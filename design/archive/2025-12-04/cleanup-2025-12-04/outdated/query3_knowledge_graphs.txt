# Knowledge Graph Construction Research

# Knowledge Graph Construction from Media Metadata: Technical Architecture and Implementation

## Overview

Building a GPU-accelerated knowledge graph from heterogeneous TV/film metadata sources requires a sophisticated pipeline that integrates entity extraction, semantic linking, and efficient graph representation learning. The modern approach to knowledge graph construction has fundamentally shifted from rule-based extraction toward **LLM-empowered and generative frameworks** that can handle the complexity of multimodal media metadata while maintaining semantic consistency across diverse sources like IMDB, TMDB, and TVDB[4].

The construction process involves three interconnected layers: ontology engineering (defining the schema), knowledge extraction (identifying entities and relationships), and knowledge fusion (resolving conflicts and enriching the graph)[4]. For media metadata specifically, this means extracting film/TV entities (actors, directors, genres, production companies), their relationships, temporal properties, and multimodal attributes (posters, trailers, reviews) into a unified semantic representation.

## ETL Pipeline Architecture for Media Metadata Ingestion

### Data Source Integration

The first stage involves collecting and normalizing metadata from heterogeneous sources. Each source—IMDB, TMDB, TVDB—maintains different schemas, identifier systems, and data quality standards.

```python
import pandas as pd
import requests
from typing import Dict, List, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class MediaMetadata:
    """Unified media metadata structure"""
    source_id: str
    source_name: str  # 'imdb', 'tmdb', 'tvdb'
    title: str
    media_type: str  # 'movie', 'tv_series'
    release_date: datetime
    genres: List[str]
    cast: List[Dict[str, str]]
    crew: List[Dict[str, str]]
    plot_summary: str
    runtime: int
    rating: float
    external_ids: Dict[str, str]
    metadata_timestamp: datetime

class MediaMetadataExtractor:
    """Extract metadata from multiple sources"""
    
    def __init__(self, tmdb_api_key: str, imdb_dataset_path: str):
        self.tmdb_api_key = tmdb_api_key
        self.imdb_dataset_path = imdb_dataset_path
        self.tmdb_base_url = "https://api.themoviedb.org/3"
        
    def extract_tmdb_movie(self, tmdb_id: int) -> MediaMetadata:
        """Extract movie metadata from TMDB API"""
        try:
            # Fetch main movie data
            movie_url = f"{self.tmdb_base_url}/movie/{tmdb_id}"
            params = {
                'api_key': self.tmdb_api_key,
                'append_to_response': 'credits,external_ids,release_dates'
            }
            response = requests.get(movie_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            # Extract cast and crew
            cast = [
                {
                    'name': person['name'],
                    'character': person.get('character', ''),
                    'tmdb_id': person['id'],
                    'order': person.get('order', 999)
                }
                for person in data.get('credits', {}).get('cast', [])[:20]
            ]
            
            crew = [
                {
                    'name': person['name'],
                    'job': person['job'],
                    'department': person['department'],
                    'tmdb_id': person['id']
                }
                for person in data.get('credits', {}).get('crew', [])
                if person['department'] in ['Directing', 'Writing', 'Production']
            ]
            
            # Parse release date
            release_date_str = data.get('release_date', '')
            release_date = datetime.strptime(release_date_str, '%Y-%m-%d') if release_date_str else None
            
            metadata = MediaMetadata(
                source_id=str(tmdb_id),
                source_name='tmdb',
                title=data.get('title', ''),
                media_type='movie',
                release_date=release_date,
                genres=[g['name'] for g in data.get('genres', [])],
                cast=cast,
                crew=crew,
                plot_summary=data.get('overview', ''),
                runtime=data.get('runtime', 0),
                rating=data.get('vote_average', 0.0),
                external_ids=data.get('external_ids', {}),
                metadata_timestamp=datetime.now()
            )
            
            logger.info(f"Extracted TMDB movie: {metadata.title} ({tmdb_id})")
            return metadata
            
        except requests.RequestException as e:
            logger.error(f"Failed to extract TMDB movie {tmdb_id}: {e}")
            raise
    
    def extract_imdb_batch(self, imdb_ids: List[str]) -> List[MediaMetadata]:
        """Extract metadata from IMDB dataset (TSV format)"""
        # Load IMDB datasets
        titles_df = pd.read_csv(
            f"{self.imdb_dataset_path}/title.basics.tsv",
            sep='\t',
            dtype={'tconst': str, 'runtimeMinutes': 'Int64', 'startYear': 'Int64'}
        )
        ratings_df = pd.read_csv(
            f"{self.imdb_dataset_path}/title.ratings.tsv",
            sep='\t'
        )
        
        # Filter for requested IDs
        titles_subset = titles_df[titles_df['tconst'].isin(imdb_ids)]
        ratings_subset = ratings_df[ratings_df['tconst'].isin(imdb_ids)]
        
        # Merge data
        merged = titles_subset.merge(ratings_subset, on='tconst', how='left')
        
        metadata_list = []
        for _, row in merged.iterrows():
            try:
                start_year = int(row['startYear']) if pd.notna(row['startYear']) and row['startYear'] != '\\N' else None
                release_date = datetime(start_year, 1, 1) if start_year else None
                
                metadata = MediaMetadata(
                    source_id=row['tconst'],
                    source_name='imdb',
                    title=row['primaryTitle'],
                    media_type='movie' if row['titleType'] == 'movie' else 'tv_series',
                    release_date=release_date,
                    genres=row['genres'].split(',') if pd.notna(row['genres']) else [],
                    cast=[],  # IMDB cast requires separate principals dataset
                    crew=[],
                    plot_summary='',
                    runtime=int(row['runtimeMinutes']) if pd.notna(row['runtimeMinutes']) else 0,
                    rating=float(row['averageRating']) if pd.notna(row['averageRating']) else 0.0,
                    external_ids={'imdb_id': row['tconst']},
                    metadata_timestamp=datetime.now()
                )
                metadata_list.append(metadata)
                
            except Exception as e:
                logger.warning(f"Failed to parse IMDB record {row['tconst']}: {e}")
                continue
        
        return metadata_list
```

### Data Normalization and Validation

```python
from enum import Enum
import hashlib
from typing import Optional

class MediaType(Enum):
    MOVIE = "movie"
    TV_SERIES = "tv_series"
    EPISODE = "episode"

class DataNormalizer:
    """Normalize and validate metadata across sources"""
    
    @staticmethod
    def normalize_title(title: str) -> str:
        """Normalize titles for comparison"""
        return title.lower().strip()
    
    @staticmethod
    def normalize_person_name(name: str) -> str:
        """Normalize person names"""
        # Remove titles, normalize spacing
        name = name.strip()
        titles = ['Dr.', 'Prof.', 'Mr.', 'Ms.', 'Mrs.', 'Sir', 'Dame']
        for title in titles:
            name = name.replace(title, '').strip()
        return ' '.join(name.split())
    
    @staticmethod
    def generate_entity_hash(entity_type: str, attributes: Dict) -> str:
        """Generate deterministic hash for entity deduplication"""
        key_str = f"{entity_type}:{json.dumps(attributes, sort_keys=True)}"
        return hashlib.sha256(key_str.encode()).hexdigest()[:16]
    
    @staticmethod
    def validate_metadata(metadata: MediaMetadata) -> Tuple[bool, List[str]]:
        """Validate metadata completeness and quality"""
        errors = []
        
        if not metadata.title or len(metadata.title.strip()) == 0:
            errors.append("Missing title")
        
        if metadata.release_date and metadata.release_date.year > datetime.now().year + 1:
            errors.append("Invalid future release date")
        
        if metadata.runtime < 0:
            errors.append("Invalid runtime")
        
        if not (0 <= metadata.rating <= 10):
            errors.append("Invalid rating range")
        
        return len(errors) == 0, errors

class MetadataQualityScorer:
    """Score metadata completeness and quality"""
    
    @staticmethod
    def calculate_completeness(metadata: MediaMetadata) -> float:
        """Calculate metadata completeness score (0-1)"""
        fields_present = 0
        total_fields = 10
        
        if metadata.title: fields_present += 1
        if metadata.release_date: fields_present += 1
        if metadata.genres: fields_present += 1
        if metadata.cast: fields_present += 1
        if metadata.crew: fields_present += 1
        if metadata.plot_summary: fields_present += 1
        if metadata.runtime > 0: fields_present += 1
        if metadata.rating > 0: fields_present += 1
        if metadata.external_ids: fields_present += 1
        if len(metadata.cast) > 5: fields_present += 1
        
        return fields_present / total_fields
    
    @staticmethod
    def calculate_consistency(metadata_list: List[MediaMetadata]) -> float:
        """Calculate consistency across multiple sources for same entity"""
        if len(metadata_list) < 2:
            return 1.0
        
        # Compare titles, release dates, genres
        title_match = all(
            DataNormalizer.normalize_title(m.title) == 
            DataNormalizer.normalize_title(metadata_list[0].title)
            for m in metadata_list
        )
        
        date_match = all(
            m.release_date == metadata_list[0].release_date
            for m in metadata_list if m.release_date
        )
        
        consistency_score = (title_match + date_match) / 2
        return consistency_score
```

## Entity Extraction and Linking

### Entity Recognition from Metadata

```python
from typing import Set, Tuple
import re

class EntityExtractor:
    """Extract entities from media metadata"""
    
    def __init__(self):
        self.entity_types = {
            'Person': ['actor', 'director', 'writer', 'producer', 'composer'],
            'Organization': ['production_company', 'studio', 'distributor'],
            'Genre': ['genre'],
            'Location': ['filming_location', 'production_location'],
            'Award': ['award', 'nomination'],
            'Franchise': ['franchise', 'series']
        }
    
    def extract_entities(self, metadata: MediaMetadata) -> Dict[str, List[Dict]]:
        """Extract all entities from metadata"""
        entities = {
            'persons': self._extract_persons(metadata),
            'organizations': self._extract_organizations(metadata),
            'genres': self._extract_genres(metadata),
            'locations': self._extract_locations(metadata),
            'media': self._extract_media_entity(metadata)
        }
        return entities
    
    def _extract_persons(self, metadata: MediaMetadata) -> List[Dict]:
        """Extract person entities (actors, crew)"""
        persons = []
        
        # Extract cast
        for actor in metadata.cast:
            person = {
                'type': 'Person',
                'name': DataNormalizer.normalize_person_name(actor['name']),
                'role': 'Actor',
                'character': actor.get('character', ''),
                'source_ids': {
                    metadata.source_name: actor.get(f'{metadata.source_name}_id', '')
                },
                'attributes': {
                    'order': actor.get('order', 999)
                }
            }
            persons.append(person)
        
        # Extract crew
        for crew_member in metadata.crew:
            person = {
                'type': 'Person',
                'name': DataNormalizer.normalize_person_name(crew_member['name']),
                'role': crew_member.get('job', 'Crew'),
                'department': crew_member.get('department', ''),
                'source_ids': {
                    metadata.source_name: crew_member.get(f'{metadata.source_name}_id', '')
                },
                'attributes': {}
            }
            persons.append(person)
        
        return persons
    
    def _extract_organizations(self, metadata: MediaMetadata) -> List[Dict]:
        """Extract organization entities"""
        # This would require additional metadata fields
        # Placeholder for production companies, studios, etc.
        return []
    
    def _extract_genres(self, metadata: MediaMetadata) -> List[Dict]:
        """Extract genre entities"""
        genres = []
        for genre in metadata.genres:
            genres.append({
                'type': 'Genre',
                'name': genre,
                'source_ids': {metadata.source_name: genre}
            })
        return genres
    
    def _extract_locations(self, metadata: MediaMetadata) -> List[Dict]:
        """Extract location entities from plot summary"""
        locations = []
        # Simple regex-based extraction; would use NER in production
        location_pattern = r'\b(?:in|at|from)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b'
        matches = re.finditer(location_pattern, metadata.plot_summary)
        for match in matches:
            locations.append({
                'type': 'Location',
                'name': match.group(1),
                'source': 'plot_summary'
            })
        return locations
    
    def _extract_media_entity(self, metadata: MediaMetadata) -> Dict:
        """Create main media entity"""
        return {
            'type': 'Media',
            'subtype': metadata.media_type,
            'title': metadata.title,
            'release_date': metadata.release_date.isoformat() if metadata.release_date else None,
            'runtime': metadata.runtime,
            'rating': metadata.rating,
            'plot_summary': metadata.plot_summary,
            'source_ids': {
                metadata.source_name: metadata.source_id,
                **metadata.external_ids
            }
        }
```

### Entity Resolution and Linking

```python
from difflib import SequenceMatcher
import numpy as np
from scipy.spatial.distance import cosine

class EntityLinker:
    """Link entities across data sources"""
    
    def __init__(self, embedding_model=None):
        self.embedding_model = embedding_model
        self.similarity_threshold = 0.85
        self.entity_index = {}  # Maps canonical IDs to entities
    
    def link_entities(self, entities_list: List[Dict[str, List[Dict]]]) -> Dict[str, str]:
        """Link entities across multiple sources"""
        # Map: (source, source_id) -> canonical_id
        entity_mapping = {}
        canonical_id_counter = 0
        
        # Group entities by type
        all_persons = []
        for entities in entities_list:
            all_persons.extend([(

## Citations
1. https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1476506/full
2. https://neo4j.com/blog/developer/llm-knowledge-graph-builder-release/
3. https://aclanthology.org/2025.emnlp-main.783/
4. https://arxiv.org/html/2510.20345v1
5. https://www.ey.com/en_us/insights/emerging-technologies/using-knowledge-graphs-to-unlock-genai-at-scale
6. https://watch.knowledgegraph.tech/kgc-2025?html=1&page=2
7. https://ceur-ws.org/Vol-3999/
8. https://www.nature.com/articles/s41467-025-62781-z
