Analyze these CUDA kernel bottlenecks for T4 GPU optimization. The kernels process media recommendation with semantic similarity on 100M vectors @ 1024-dim.

CRITICAL PERFORMANCE ISSUES:

1. TENSOR CORES NOT UTILIZED (semantic_similarity_fp16.cu)
Current: Scalar FP16 operations in loops
Problem: Missing WMMA API for 16x16 matrix ops
Impact: 8-10x throughput loss on T4 tensor cores

2. UNCOALESCED MEMORY ACCESS (all kernels)
Current: Random index patterns from item_pairs arrays
Problem: Breaking memory coalescing, 60 GB/s vs 320 GB/s theoretical
Impact: 75% bandwidth efficiency loss

3. O(N) LINEAR SEARCH (ontology_reasoning.cu)
Current: Linear search through 10K nodes for every constraint
Problem: 500M unnecessary comparisons
Impact: 95% of kernel execution time

4. WARP DIVERGENCE (graph_search.cu)
Current: Variable iteration counts in nested loops
Problem: Different threads take different branches
Impact: 50-70% warp efficiency loss

5. QUADRATIC COMPLEXITY (semantic_similarity.cu)
Current: All-pairs comparison without spatial partitioning
Problem: 100M comparisons for 10K items
Impact: Prevents scaling to production

QUESTIONS FOR REASONING:

1. How should we restructure memory layout to achieve coalesced access while maintaining random pair sampling?

2. What's the optimal WMMA tiling strategy for 1024-dim vectors on T4 tensor cores?

3. Should we use hash tables or sorted arrays for O(1) ontology lookups?

4. How can we eliminate warp divergence in graph traversal while maintaining correctness?

5. What spatial data structure (KD-tree, LSH, HNSW) best breaks O(NÂ²) complexity for semantic search?

Provide root cause analysis, specific optimizations, and expected speedup estimates.