# Makefile for GPU Kernels - A100 Optimized
# NVIDIA A100 (Ampere sm_80) with TF32/FP16/BF16 Tensor Core Support
#
# A100 Specifications (vs T4):
#   Architecture: Ampere (sm_80) vs Turing (sm_75)
#   CUDA cores: 6912 (108 SMs × 64 cores) vs 2560 (40 SMs × 64 cores) = 2.7x
#   Tensor cores: 432 (TF32/FP16/BF16/FP64) vs 320 (FP16 only)
#   Memory: 40GB HBM2e @ 1.6 TB/s vs 16GB GDDR6 @ 320 GB/s = 5x bandwidth
#   FP16 Tensor: 312 TFLOPS vs 65 TFLOPS = 4.8x
#   TF32 Tensor: 156 TFLOPS (not available on T4) = 19x vs FP32
#   FP32: 19.5 TFLOPS vs 8.1 TFLOPS = 2.4x

# Compiler
NVCC := nvcc

# A100-specific architecture flags (Ampere sm_80)
# Also support A30 (sm_80) and future H100 (sm_90)
CUDA_ARCH := -arch=sm_80 -gencode arch=compute_80,code=sm_80 \
             -gencode arch=compute_80,code=compute_80

# A100 Optimization flags
# - TF32 enabled by default (19x speedup for matrix ops)
# - BF16 support for training workloads
# - Async copy support (cp.async)
# - Enhanced L2 cache utilization (40MB vs 5MB on T4)
# - Device Link Time Optimization (-dlto) for cross-kernel optimization
CUDA_FLAGS := -O3 -std=c++17 \
              --use_fast_math \
              --expt-relaxed-constexpr \
              --extended-lambda \
              -Xptxas -O3,-v \
              -Xcompiler -O3,-march=native,-fPIC \
              -lineinfo \
              -dlto \
              --threads 8

# A100-specific tensor core and memory flags
TENSOR_FLAGS := -DUSE_TENSOR_CORES=1 \
                -DUSE_TF32=1 \
                -DUSE_FP16=1 \
                -DUSE_BF16=1 \
                -DUSE_ASYNC_COPY=1 \
                -DA100_OPTIMIZED=1

# A100 register/occupancy tuning
# A100 has 65536 registers per SM (vs 65536 on T4)
# Higher register count = more complex kernels per warp
OCCUPANCY_FLAGS := -maxrregcount=255

# Include paths
INCLUDE := -I./kernels -I/usr/local/cuda/include

# Libraries
LIBS := -lcudart -lcuda -lcublas -lcudnn

# Directories
KERNEL_DIR := .
BUILD_DIR := build_a100
PTX_DIR := ../../../target/ptx_a100

# Source files - A100 optimized kernels
# Core kernels with A100-specific optimizations
PTX_KERNELS := \
	$(KERNEL_DIR)/semantic_similarity.cu \
	$(KERNEL_DIR)/semantic_similarity_fp16.cu \
	$(KERNEL_DIR)/semantic_similarity_tf32.cu \
	$(KERNEL_DIR)/semantic_similarity_fp16_tensor_cores.cu \
	$(KERNEL_DIR)/graph_search.cu \
	$(KERNEL_DIR)/ontology_reasoning.cu \
	$(KERNEL_DIR)/hybrid_sssp.cu \
	$(KERNEL_DIR)/product_quantization.cu

# All kernels including experimental
ALL_KERNELS := $(PTX_KERNELS) \
	$(KERNEL_DIR)/unified_pipeline.cu \
	$(KERNEL_DIR)/memory_layout.cu \
	$(KERNEL_DIR)/sorted_similarity.cu \
	$(KERNEL_DIR)/lsh_gpu.cu \
	$(KERNEL_DIR)/hybrid_index.cu

# Object files
OBJECTS := $(patsubst $(KERNEL_DIR)/%.cu,$(BUILD_DIR)/%.o,$(ALL_KERNELS))

# ============================================================================
# BUILD TARGETS
# ============================================================================

all: dirs ptx $(BUILD_DIR)/libkernels_a100.a benchmark-a100

dirs:
	@mkdir -p $(BUILD_DIR) $(PTX_DIR)

# Compile individual kernels with A100 optimizations
$(BUILD_DIR)/%.o: $(KERNEL_DIR)/%.cu | dirs
	$(NVCC) $(CUDA_ARCH) $(CUDA_FLAGS) $(TENSOR_FLAGS) $(OCCUPANCY_FLAGS) $(INCLUDE) \
		-dc $< -o $@
	@echo "✓ Compiled (A100): $< -> $@"

# Link into static library with device link optimization
$(BUILD_DIR)/libkernels_a100.a: $(OBJECTS)
	$(NVCC) $(CUDA_ARCH) $(CUDA_FLAGS) -dlink $(OBJECTS) -o $(BUILD_DIR)/device_link.o
	ar rcs $@ $(OBJECTS) $(BUILD_DIR)/device_link.o
	@echo "✓ Built A100 kernel library: $@"

# ============================================================================
# PTX GENERATION (for Rust FFI)
# ============================================================================

NVCC_PTX_FLAGS := -ptx -O3 -std=c++17 -arch=sm_80 \
                  --use_fast_math --expt-relaxed-constexpr --extended-lambda \
                  -Xptxas -v -Xcompiler -fPIC \
                  $(TENSOR_FLAGS)

ptx: $(PTX_KERNELS) | dirs
	@echo "==================================================================="
	@echo "Compiling CUDA kernels to PTX for Rust FFI"
	@echo "Target: A100 GPU (sm_80) with TF32/FP16/BF16 Tensor Cores"
	@echo "==================================================================="
	@for kernel in $(PTX_KERNELS); do \
		base=$$(basename $$kernel .cu); \
		echo "Compiling $$base.cu -> $$base.ptx"; \
		$(NVCC) $(NVCC_PTX_FLAGS) $(INCLUDE) \
			$$kernel -o $(PTX_DIR)/$$base.ptx 2>&1 | grep -E "(ptxas info|warning|error)" || true; \
		if [ -f $(PTX_DIR)/$$base.ptx ]; then \
			echo "✓ Generated: $(PTX_DIR)/$$base.ptx ($$(wc -c < $(PTX_DIR)/$$base.ptx) bytes)"; \
		else \
			echo "⚠ Skipped: $$base.ptx (compile issues)"; \
		fi; \
	done
	@echo "==================================================================="
	@echo "PTX compilation complete. Files in $(PTX_DIR)/"
	@ls -lh $(PTX_DIR)/*.ptx 2>/dev/null || echo "No PTX files generated"
	@echo "==================================================================="

# ============================================================================
# A100-SPECIFIC BENCHMARKS
# ============================================================================

benchmark-a100: $(BUILD_DIR)/benchmark_a100
	@echo "A100 benchmark binary ready: $(BUILD_DIR)/benchmark_a100"

$(BUILD_DIR)/benchmark_a100: benchmark_algorithms.cu hnsw_gpu.cuh lsh_gpu.cu | dirs
	$(NVCC) $(CUDA_ARCH) $(CUDA_FLAGS) $(TENSOR_FLAGS) $(INCLUDE) \
		benchmark_algorithms.cu -o $@ $(LIBS) -lcurand
	@echo "✓ A100 benchmark compiled: $@"

run-benchmark: benchmark-a100
	@echo "==================================================================="
	@echo "Running A100 Benchmark Suite"
	@echo "Expected performance vs T4:"
	@echo "  - TF32 Tensor ops: 19x faster"
	@echo "  - FP16 Tensor ops: 4.8x faster"
	@echo "  - Memory bandwidth: 5x faster"
	@echo "  - Overall: 5-10x end-to-end speedup"
	@echo "==================================================================="
	./$(BUILD_DIR)/benchmark_a100

# ============================================================================
# PROFILING (A100-specific metrics)
# ============================================================================

profile: benchmark-a100
	@echo "Profiling A100 kernels with Nsight Compute..."
	ncu --target-processes all \
	    --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,\
dram__throughput.avg.pct_of_peak_sustained_elapsed,\
l2_utilization,\
smsp__sass_thread_inst_executed_op_fp16_pred_on.sum,\
smsp__sass_thread_inst_executed_op_fp32_pred_on.sum,\
sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_elapsed \
	    --launch-skip 0 --launch-count 10 \
	    ./$(BUILD_DIR)/benchmark_a100

# Check TF32 utilization
check-tf32: benchmark-a100
	@echo "Checking TF32 Tensor Core utilization..."
	ncu --metrics sm__pipe_tensor_op_hmma_cycles_active.avg.pct_of_peak \
	    ./$(BUILD_DIR)/benchmark_a100

# Memory bandwidth analysis (target: >1 TB/s)
check-bandwidth: benchmark-a100
	@echo "Analyzing A100 memory bandwidth (target: >1 TB/s)..."
	ncu --metrics dram__bytes.sum.per_second \
	    ./$(BUILD_DIR)/benchmark_a100

# L2 cache efficiency (40MB L2 on A100)
check-l2: benchmark-a100
	@echo "Analyzing L2 cache utilization (40MB available)..."
	ncu --metrics l2_cache_hit_rate,l2_tex_hit_rate \
	    ./$(BUILD_DIR)/benchmark_a100

# Register usage analysis
check-registers: $(PTX_KERNELS) | dirs
	@echo "=== A100 Register Usage Analysis ==="
	@for kernel in $(PTX_KERNELS); do \
		base=$$(basename $$kernel .cu); \
		echo "--- $$base ---"; \
		$(NVCC) $(CUDA_ARCH) $(CUDA_FLAGS) $(TENSOR_FLAGS) $(INCLUDE) \
			--ptxas-options=-v -c $$kernel -o /dev/null 2>&1 | grep -E "(registers|spills|shared|cmem)"; \
	done

# ============================================================================
# SPECIALIZED BUILDS
# ============================================================================

# TF32-only build (maximum performance for inference)
tf32: TENSOR_FLAGS += -DFORCE_TF32=1
tf32: all

# FP16-only build (maximum tensor core throughput)
fp16: TENSOR_FLAGS += -DFORCE_FP16=1
fp16: all

# BF16 build (for training workloads)
bf16: TENSOR_FLAGS += -DFORCE_BF16=1
bf16: all

# Multi-GPU build with NCCL
multi-gpu: LIBS += -lnccl
multi-gpu: TENSOR_FLAGS += -DMULTI_GPU=1
multi-gpu: all

# Debug build
debug: CUDA_FLAGS := -g -G -lineinfo -DDEBUG=1
debug: all

# ============================================================================
# UTILITIES
# ============================================================================

clean:
	rm -rf $(BUILD_DIR) $(PTX_DIR)
	@echo "Cleaned A100 build artifacts"

help:
	@echo "A100-Optimized GPU Kernel Makefile"
	@echo "==================================="
	@echo ""
	@echo "Main Targets:"
	@echo "  all              - Build A100-optimized kernel library (default)"
	@echo "  ptx              - Generate PTX for Rust FFI"
	@echo "  run-benchmark    - Run A100 performance benchmarks"
	@echo "  clean            - Remove build artifacts"
	@echo ""
	@echo "Profiling:"
	@echo "  profile          - Full Nsight Compute analysis"
	@echo "  check-tf32       - Verify TF32 tensor core usage"
	@echo "  check-bandwidth  - Memory bandwidth analysis"
	@echo "  check-l2         - L2 cache efficiency"
	@echo "  check-registers  - Register pressure analysis"
	@echo ""
	@echo "Specialized Builds:"
	@echo "  tf32             - TF32-only (max inference perf)"
	@echo "  fp16             - FP16-only (max tensor throughput)"
	@echo "  bf16             - BF16 (training workloads)"
	@echo "  multi-gpu        - NCCL multi-GPU support"
	@echo "  debug            - Debug symbols enabled"
	@echo ""
	@echo "A100 Specifications:"
	@echo "  Architecture: Ampere (sm_80)"
	@echo "  CUDA cores: 6912 (108 SMs × 64 cores/SM)"
	@echo "  Tensor cores: 432 (TF32/FP16/BF16/FP64)"
	@echo "  Memory: 40GB HBM2e @ 1.6 TB/s"
	@echo "  TF32 peak: 156 TFLOPS"
	@echo "  FP16 peak: 312 TFLOPS"
	@echo "  FP32 peak: 19.5 TFLOPS"
	@echo ""
	@echo "Expected Performance vs T4:"
	@echo "  TF32 Tensor: 19x faster"
	@echo "  FP16 Tensor: 4.8x faster"
	@echo "  Memory BW: 5x faster"
	@echo "  Compute: 2.4x faster"
	@echo ""

.PHONY: all dirs ptx benchmark-a100 run-benchmark profile check-tf32 check-bandwidth check-l2 check-registers tf32 fp16 bf16 multi-gpu debug clean help
