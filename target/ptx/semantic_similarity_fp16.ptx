//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_75
.address_size 64

	// .globl	compute_multimodal_similarity_fp16_t4
// _ZZ22topk_selection_warp_t4E13shared_scores has been demoted
// _ZZ22topk_selection_warp_t4E14shared_indices has been demoted

.visible .entry compute_multimodal_similarity_fp16_t4(
	.param .u64 compute_multimodal_similarity_fp16_t4_param_0,
	.param .u64 compute_multimodal_similarity_fp16_t4_param_1,
	.param .u64 compute_multimodal_similarity_fp16_t4_param_2,
	.param .u64 compute_multimodal_similarity_fp16_t4_param_3,
	.param .u64 compute_multimodal_similarity_fp16_t4_param_4,
	.param .u64 compute_multimodal_similarity_fp16_t4_param_5,
	.param .u32 compute_multimodal_similarity_fp16_t4_param_6,
	.param .u32 compute_multimodal_similarity_fp16_t4_param_7,
	.param .u32 compute_multimodal_similarity_fp16_t4_param_8,
	.param .u32 compute_multimodal_similarity_fp16_t4_param_9,
	.param .f32 compute_multimodal_similarity_fp16_t4_param_10,
	.param .f32 compute_multimodal_similarity_fp16_t4_param_11,
	.param .f32 compute_multimodal_similarity_fp16_t4_param_12
)
{
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<550>;
	.reg .b32 	%r<276>;
	.reg .b64 	%rd<61>;


	ld.param.u64 	%rd23, [compute_multimodal_similarity_fp16_t4_param_0];
	ld.param.u64 	%rd24, [compute_multimodal_similarity_fp16_t4_param_1];
	ld.param.u64 	%rd25, [compute_multimodal_similarity_fp16_t4_param_2];
	ld.param.u64 	%rd26, [compute_multimodal_similarity_fp16_t4_param_3];
	ld.param.u64 	%rd27, [compute_multimodal_similarity_fp16_t4_param_4];
	ld.param.u64 	%rd28, [compute_multimodal_similarity_fp16_t4_param_5];
	ld.param.u32 	%r35, [compute_multimodal_similarity_fp16_t4_param_6];
	ld.param.u32 	%r36, [compute_multimodal_similarity_fp16_t4_param_7];
	ld.param.u32 	%r37, [compute_multimodal_similarity_fp16_t4_param_8];
	ld.param.u32 	%r38, [compute_multimodal_similarity_fp16_t4_param_9];
	ld.param.f32 	%f117, [compute_multimodal_similarity_fp16_t4_param_10];
	ld.param.f32 	%f118, [compute_multimodal_similarity_fp16_t4_param_11];
	ld.param.f32 	%f119, [compute_multimodal_similarity_fp16_t4_param_12];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r39, %ctaid.x;
	mov.u32 	%r40, %tid.x;
	mad.lo.s32 	%r266, %r39, %r1, %r40;
	setp.ge.s32 	%p4, %r266, %r35;
	@%p4 bra 	$L__BB0_47;

	cvta.to.global.u64 	%rd1, %rd25;
	cvta.to.global.u64 	%rd2, %rd24;
	cvta.to.global.u64 	%rd3, %rd23;
	setp.gt.ftz.f32 	%p5, %f117, 0f00000000;
	setp.ne.s64 	%p6, %rd23, 0;
	and.pred  	%p1, %p6, %p5;
	setp.ne.s64 	%p7, %rd24, 0;
	setp.gt.ftz.f32 	%p8, %f118, 0f00000000;
	and.pred  	%p2, %p7, %p8;
	and.b32  	%r3, %r36, 1;
	setp.ne.s64 	%p9, %rd25, 0;
	setp.gt.ftz.f32 	%p10, %f119, 0f00000000;
	and.pred  	%p3, %p9, %p10;
	add.s32 	%r41, %r36, -1;
	cvt.s64.s32 	%rd4, %r41;
	and.b32  	%r4, %r37, 1;
	add.s32 	%r42, %r37, -1;
	cvt.s64.s32 	%rd5, %r42;
	and.b32  	%r5, %r38, 1;
	add.s32 	%r43, %r38, -1;
	cvt.s64.s32 	%rd6, %r43;
	add.ftz.f32 	%f1, %f117, 0f00000000;
	shr.u32 	%r44, %r36, 31;
	add.s32 	%r45, %r36, %r44;
	shr.s32 	%r46, %r45, 1;
	add.s32 	%r6, %r46, -1;
	shr.u32 	%r47, %r37, 31;
	add.s32 	%r48, %r37, %r47;
	shr.s32 	%r49, %r48, 1;
	add.s32 	%r7, %r49, -1;
	shr.u32 	%r50, %r38, 31;
	add.s32 	%r51, %r38, %r50;
	shr.s32 	%r52, %r51, 1;
	add.s32 	%r8, %r52, -1;
	and.b32  	%r9, %r46, 3;
	sub.s32 	%r10, %r46, %r9;
	and.b32  	%r11, %r49, 3;
	sub.s32 	%r12, %r49, %r11;
	and.b32  	%r13, %r52, 3;
	sub.s32 	%r14, %r52, %r13;
	mov.u32 	%r53, %nctaid.x;
	mul.lo.s32 	%r15, %r1, %r53;
	cvta.to.global.u64 	%rd7, %rd26;
	cvta.to.global.u64 	%rd8, %rd27;
	cvta.to.global.u64 	%rd9, %rd28;
	not.pred 	%p11, %p1;
	shl.b64 	%rd38, %rd4, 1;
	not.pred 	%p35, %p2;
	shl.b64 	%rd47, %rd5, 1;
	not.pred 	%p59, %p3;
	shl.b64 	%rd56, %rd6, 1;

$L__BB0_2:
	cvt.s64.s32 	%rd10, %r266;
	mul.wide.s32 	%rd29, %r266, 4;
	add.s64 	%rd30, %rd7, %rd29;
	ld.global.nc.u32 	%r17, [%rd30];
	add.s64 	%rd31, %rd8, %rd29;
	ld.global.nc.u32 	%r18, [%rd31];
	mov.f32 	%f529, 0f00000000;
	mov.f32 	%f530, %f529;
	@%p11 bra 	$L__BB0_16;

	ld.param.u32 	%r263, [compute_multimodal_similarity_fp16_t4_param_7];
	setp.lt.s32 	%p12, %r263, 2;
	mul.lo.s32 	%r54, %r17, %r263;
	mul.wide.s32 	%rd32, %r54, 2;
	add.s64 	%rd11, %rd3, %rd32;
	mul.lo.s32 	%r55, %r18, %r263;
	mul.wide.s32 	%rd33, %r55, 2;
	add.s64 	%rd12, %rd3, %rd33;
	mov.f32 	%f504, 0f00000000;
	mov.f32 	%f505, %f504;
	mov.f32 	%f506, %f504;
	@%p12 bra 	$L__BB0_11;

	setp.lt.u32 	%p13, %r6, 3;
	mov.f32 	%f506, 0f00000000;
	mov.u32 	%r269, 0;
	mov.f32 	%f505, %f506;
	mov.f32 	%f504, %f506;
	@%p13 bra 	$L__BB0_7;

	mov.f32 	%f506, 0f00000000;
	mov.u32 	%r269, 0;
	mov.u32 	%r268, %r10;

$L__BB0_6:
	.pragma "nounroll";
	mul.wide.s32 	%rd34, %r269, 4;
	add.s64 	%rd35, %rd11, %rd34;
	ld.global.nc.u32 	%r58, [%rd35];
	add.s64 	%rd36, %rd12, %rd34;
	ld.global.nc.u32 	%r60, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f132, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f133, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f134, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f135, high;}

	// end inline asm
	mul.ftz.f32 	%f148, %f133, %f135;
	fma.rn.ftz.f32 	%f149, %f132, %f134, %f148;
	add.ftz.f32 	%f150, %f505, %f149;
	mul.ftz.f32 	%f151, %f133, %f133;
	fma.rn.ftz.f32 	%f152, %f132, %f132, %f151;
	add.ftz.f32 	%f153, %f504, %f152;
	mul.ftz.f32 	%f154, %f135, %f135;
	fma.rn.ftz.f32 	%f155, %f134, %f134, %f154;
	add.ftz.f32 	%f156, %f506, %f155;
	ld.global.nc.u32 	%r62, [%rd35+4];
	ld.global.nc.u32 	%r64, [%rd36+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f136, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f137, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f138, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f139, high;}

	// end inline asm
	mul.ftz.f32 	%f157, %f137, %f139;
	fma.rn.ftz.f32 	%f158, %f136, %f138, %f157;
	add.ftz.f32 	%f159, %f150, %f158;
	mul.ftz.f32 	%f160, %f137, %f137;
	fma.rn.ftz.f32 	%f161, %f136, %f136, %f160;
	add.ftz.f32 	%f162, %f153, %f161;
	mul.ftz.f32 	%f163, %f139, %f139;
	fma.rn.ftz.f32 	%f164, %f138, %f138, %f163;
	add.ftz.f32 	%f165, %f156, %f164;
	ld.global.nc.u32 	%r66, [%rd35+8];
	ld.global.nc.u32 	%r68, [%rd36+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f140, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f141, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f142, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f143, high;}

	// end inline asm
	mul.ftz.f32 	%f166, %f141, %f143;
	fma.rn.ftz.f32 	%f167, %f140, %f142, %f166;
	add.ftz.f32 	%f168, %f159, %f167;
	mul.ftz.f32 	%f169, %f141, %f141;
	fma.rn.ftz.f32 	%f170, %f140, %f140, %f169;
	add.ftz.f32 	%f171, %f162, %f170;
	mul.ftz.f32 	%f172, %f143, %f143;
	fma.rn.ftz.f32 	%f173, %f142, %f142, %f172;
	add.ftz.f32 	%f174, %f165, %f173;
	ld.global.nc.u32 	%r70, [%rd35+12];
	ld.global.nc.u32 	%r72, [%rd36+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f144, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f145, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f146, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f147, high;}

	// end inline asm
	mul.ftz.f32 	%f175, %f145, %f147;
	fma.rn.ftz.f32 	%f176, %f144, %f146, %f175;
	add.ftz.f32 	%f505, %f168, %f176;
	mul.ftz.f32 	%f177, %f145, %f145;
	fma.rn.ftz.f32 	%f178, %f144, %f144, %f177;
	add.ftz.f32 	%f504, %f171, %f178;
	mul.ftz.f32 	%f179, %f147, %f147;
	fma.rn.ftz.f32 	%f180, %f146, %f146, %f179;
	add.ftz.f32 	%f506, %f174, %f180;
	add.s32 	%r269, %r269, 4;
	add.s32 	%r268, %r268, -4;
	setp.ne.s32 	%p14, %r268, 0;
	@%p14 bra 	$L__BB0_6;

$L__BB0_7:
	setp.eq.s32 	%p15, %r9, 0;
	@%p15 bra 	$L__BB0_11;

	mul.wide.s32 	%rd37, %r269, 4;
	add.s64 	%rd13, %rd11, %rd37;
	ld.global.nc.u32 	%r74, [%rd13];
	add.s64 	%rd14, %rd12, %rd37;
	ld.global.nc.u32 	%r76, [%rd14];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f181, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f182, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f183, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f184, high;}

	// end inline asm
	mul.ftz.f32 	%f185, %f182, %f184;
	fma.rn.ftz.f32 	%f186, %f181, %f183, %f185;
	add.ftz.f32 	%f505, %f505, %f186;
	mul.ftz.f32 	%f187, %f182, %f182;
	fma.rn.ftz.f32 	%f188, %f181, %f181, %f187;
	add.ftz.f32 	%f504, %f504, %f188;
	mul.ftz.f32 	%f189, %f184, %f184;
	fma.rn.ftz.f32 	%f190, %f183, %f183, %f189;
	add.ftz.f32 	%f506, %f506, %f190;
	setp.eq.s32 	%p16, %r9, 1;
	@%p16 bra 	$L__BB0_11;

	setp.eq.s32 	%p17, %r9, 2;
	ld.global.nc.u32 	%r78, [%rd13+4];
	ld.global.nc.u32 	%r80, [%rd14+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f191, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f192, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r80;
  cvt.f32.f16 %f193, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r80;
  cvt.f32.f16 %f194, high;}

	// end inline asm
	mul.ftz.f32 	%f195, %f192, %f194;
	fma.rn.ftz.f32 	%f196, %f191, %f193, %f195;
	add.ftz.f32 	%f505, %f505, %f196;
	mul.ftz.f32 	%f197, %f192, %f192;
	fma.rn.ftz.f32 	%f198, %f191, %f191, %f197;
	add.ftz.f32 	%f504, %f504, %f198;
	mul.ftz.f32 	%f199, %f194, %f194;
	fma.rn.ftz.f32 	%f200, %f193, %f193, %f199;
	add.ftz.f32 	%f506, %f506, %f200;
	@%p17 bra 	$L__BB0_11;

	ld.global.nc.u32 	%r82, [%rd13+8];
	ld.global.nc.u32 	%r84, [%rd14+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r82;
  cvt.f32.f16 %f201, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r82;
  cvt.f32.f16 %f202, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r84;
  cvt.f32.f16 %f203, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r84;
  cvt.f32.f16 %f204, high;}

	// end inline asm
	mul.ftz.f32 	%f205, %f202, %f204;
	fma.rn.ftz.f32 	%f206, %f201, %f203, %f205;
	add.ftz.f32 	%f505, %f505, %f206;
	mul.ftz.f32 	%f207, %f202, %f202;
	fma.rn.ftz.f32 	%f208, %f201, %f201, %f207;
	add.ftz.f32 	%f504, %f504, %f208;
	mul.ftz.f32 	%f209, %f204, %f204;
	fma.rn.ftz.f32 	%f210, %f203, %f203, %f209;
	add.ftz.f32 	%f506, %f506, %f210;

$L__BB0_11:
	setp.eq.s32 	%p18, %r3, 0;
	@%p18 bra 	$L__BB0_13;

	add.s64 	%rd39, %rd11, %rd38;
	ld.global.nc.u16 	%rs1, [%rd39];
	// begin inline asm
	{  cvt.f32.f16 %f211, %rs1;}

	// end inline asm
	add.s64 	%rd40, %rd12, %rd38;
	ld.global.nc.u16 	%rs2, [%rd40];
	// begin inline asm
	{  cvt.f32.f16 %f212, %rs2;}

	// end inline asm
	fma.rn.ftz.f32 	%f505, %f211, %f212, %f505;
	fma.rn.ftz.f32 	%f504, %f211, %f211, %f504;
	fma.rn.ftz.f32 	%f506, %f212, %f212, %f506;

$L__BB0_13:
	mov.b32 	%r86, %f505;
	mov.u32 	%r87, 2;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.down.b32 	%r91|%p19, %r86, %r89, %r88, %r90;
	mov.b32 	%f214, %r91;
	add.ftz.f32 	%f215, %f505, %f214;
	mov.b32 	%r92, %f504;
	shfl.sync.down.b32 	%r93|%p20, %r92, %r89, %r88, %r90;
	mov.b32 	%f216, %r93;
	add.ftz.f32 	%f217, %f504, %f216;
	mov.b32 	%r94, %f506;
	shfl.sync.down.b32 	%r95|%p21, %r94, %r89, %r88, %r90;
	mov.b32 	%f218, %r95;
	add.ftz.f32 	%f219, %f506, %f218;
	mov.b32 	%r96, %f215;
	mov.u32 	%r97, 8;
	shfl.sync.down.b32 	%r98|%p22, %r96, %r97, %r88, %r90;
	mov.b32 	%f220, %r98;
	add.ftz.f32 	%f221, %f215, %f220;
	mov.b32 	%r99, %f217;
	shfl.sync.down.b32 	%r100|%p23, %r99, %r97, %r88, %r90;
	mov.b32 	%f222, %r100;
	add.ftz.f32 	%f223, %f217, %f222;
	mov.b32 	%r101, %f219;
	shfl.sync.down.b32 	%r102|%p24, %r101, %r97, %r88, %r90;
	mov.b32 	%f224, %r102;
	add.ftz.f32 	%f225, %f219, %f224;
	mov.b32 	%r103, %f221;
	mov.u32 	%r104, 4;
	shfl.sync.down.b32 	%r105|%p25, %r103, %r104, %r88, %r90;
	mov.b32 	%f226, %r105;
	add.ftz.f32 	%f227, %f221, %f226;
	mov.b32 	%r106, %f223;
	shfl.sync.down.b32 	%r107|%p26, %r106, %r104, %r88, %r90;
	mov.b32 	%f228, %r107;
	add.ftz.f32 	%f229, %f223, %f228;
	mov.b32 	%r108, %f225;
	shfl.sync.down.b32 	%r109|%p27, %r108, %r104, %r88, %r90;
	mov.b32 	%f230, %r109;
	add.ftz.f32 	%f231, %f225, %f230;
	mov.b32 	%r110, %f227;
	shfl.sync.down.b32 	%r111|%p28, %r110, %r87, %r88, %r90;
	mov.b32 	%f232, %r111;
	add.ftz.f32 	%f233, %f227, %f232;
	mov.b32 	%r112, %f229;
	shfl.sync.down.b32 	%r113|%p29, %r112, %r87, %r88, %r90;
	mov.b32 	%f234, %r113;
	add.ftz.f32 	%f235, %f229, %f234;
	mov.b32 	%r114, %f231;
	shfl.sync.down.b32 	%r115|%p30, %r114, %r87, %r88, %r90;
	mov.b32 	%f236, %r115;
	add.ftz.f32 	%f237, %f231, %f236;
	mov.b32 	%r116, %f233;
	mov.u32 	%r117, 1;
	shfl.sync.down.b32 	%r118|%p31, %r116, %r117, %r88, %r90;
	mov.b32 	%f238, %r118;
	add.ftz.f32 	%f32, %f233, %f238;
	mov.b32 	%r119, %f235;
	shfl.sync.down.b32 	%r120|%p32, %r119, %r117, %r88, %r90;
	mov.b32 	%f239, %r120;
	add.ftz.f32 	%f240, %f235, %f239;
	mov.b32 	%r121, %f237;
	shfl.sync.down.b32 	%r122|%p33, %r121, %r117, %r88, %r90;
	mov.b32 	%f241, %r122;
	add.ftz.f32 	%f242, %f237, %f241;
	sqrt.approx.ftz.f32 	%f243, %f240;
	sqrt.approx.ftz.f32 	%f244, %f242;
	mul.ftz.f32 	%f33, %f243, %f244;
	setp.lt.ftz.f32 	%p34, %f33, 0f358637BD;
	mov.f32 	%f510, 0f00000000;
	@%p34 bra 	$L__BB0_15;

	div.approx.ftz.f32 	%f510, %f32, %f33;

$L__BB0_15:
	ld.param.f32 	%f492, [compute_multimodal_similarity_fp16_t4_param_10];
	fma.rn.ftz.f32 	%f529, %f510, %f492, 0f00000000;
	mov.f32 	%f530, %f1;

$L__BB0_16:
	@%p35 bra 	$L__BB0_30;

	ld.param.u32 	%r265, [compute_multimodal_similarity_fp16_t4_param_8];
	setp.lt.s32 	%p36, %r265, 2;
	mul.lo.s32 	%r123, %r17, %r265;
	mul.wide.s32 	%rd41, %r123, 2;
	add.s64 	%rd15, %rd2, %rd41;
	mul.lo.s32 	%r124, %r18, %r265;
	mul.wide.s32 	%rd42, %r124, 2;
	add.s64 	%rd16, %rd2, %rd42;
	mov.f32 	%f522, 0f00000000;
	mov.f32 	%f523, %f522;
	mov.f32 	%f524, %f522;
	@%p36 bra 	$L__BB0_25;

	setp.lt.u32 	%p37, %r7, 3;
	mov.f32 	%f524, 0f00000000;
	mov.u32 	%r272, 0;
	mov.f32 	%f523, %f524;
	mov.f32 	%f522, %f524;
	@%p37 bra 	$L__BB0_21;

	mov.f32 	%f524, 0f00000000;
	mov.u32 	%r272, 0;
	mov.u32 	%r271, %r12;

$L__BB0_20:
	.pragma "nounroll";
	mul.wide.s32 	%rd43, %r272, 4;
	add.s64 	%rd44, %rd15, %rd43;
	ld.global.nc.u32 	%r127, [%rd44];
	add.s64 	%rd45, %rd16, %rd43;
	ld.global.nc.u32 	%r129, [%rd45];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r127;
  cvt.f32.f16 %f255, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r127;
  cvt.f32.f16 %f256, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r129;
  cvt.f32.f16 %f257, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r129;
  cvt.f32.f16 %f258, high;}

	// end inline asm
	mul.ftz.f32 	%f271, %f256, %f258;
	fma.rn.ftz.f32 	%f272, %f255, %f257, %f271;
	add.ftz.f32 	%f273, %f523, %f272;
	mul.ftz.f32 	%f274, %f256, %f256;
	fma.rn.ftz.f32 	%f275, %f255, %f255, %f274;
	add.ftz.f32 	%f276, %f522, %f275;
	mul.ftz.f32 	%f277, %f258, %f258;
	fma.rn.ftz.f32 	%f278, %f257, %f257, %f277;
	add.ftz.f32 	%f279, %f524, %f278;
	ld.global.nc.u32 	%r131, [%rd44+4];
	ld.global.nc.u32 	%r133, [%rd45+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r131;
  cvt.f32.f16 %f259, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r131;
  cvt.f32.f16 %f260, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r133;
  cvt.f32.f16 %f261, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r133;
  cvt.f32.f16 %f262, high;}

	// end inline asm
	mul.ftz.f32 	%f280, %f260, %f262;
	fma.rn.ftz.f32 	%f281, %f259, %f261, %f280;
	add.ftz.f32 	%f282, %f273, %f281;
	mul.ftz.f32 	%f283, %f260, %f260;
	fma.rn.ftz.f32 	%f284, %f259, %f259, %f283;
	add.ftz.f32 	%f285, %f276, %f284;
	mul.ftz.f32 	%f286, %f262, %f262;
	fma.rn.ftz.f32 	%f287, %f261, %f261, %f286;
	add.ftz.f32 	%f288, %f279, %f287;
	ld.global.nc.u32 	%r135, [%rd44+8];
	ld.global.nc.u32 	%r137, [%rd45+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r135;
  cvt.f32.f16 %f263, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r135;
  cvt.f32.f16 %f264, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r137;
  cvt.f32.f16 %f265, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r137;
  cvt.f32.f16 %f266, high;}

	// end inline asm
	mul.ftz.f32 	%f289, %f264, %f266;
	fma.rn.ftz.f32 	%f290, %f263, %f265, %f289;
	add.ftz.f32 	%f291, %f282, %f290;
	mul.ftz.f32 	%f292, %f264, %f264;
	fma.rn.ftz.f32 	%f293, %f263, %f263, %f292;
	add.ftz.f32 	%f294, %f285, %f293;
	mul.ftz.f32 	%f295, %f266, %f266;
	fma.rn.ftz.f32 	%f296, %f265, %f265, %f295;
	add.ftz.f32 	%f297, %f288, %f296;
	ld.global.nc.u32 	%r139, [%rd44+12];
	ld.global.nc.u32 	%r141, [%rd45+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r139;
  cvt.f32.f16 %f267, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r139;
  cvt.f32.f16 %f268, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r141;
  cvt.f32.f16 %f269, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r141;
  cvt.f32.f16 %f270, high;}

	// end inline asm
	mul.ftz.f32 	%f298, %f268, %f270;
	fma.rn.ftz.f32 	%f299, %f267, %f269, %f298;
	add.ftz.f32 	%f523, %f291, %f299;
	mul.ftz.f32 	%f300, %f268, %f268;
	fma.rn.ftz.f32 	%f301, %f267, %f267, %f300;
	add.ftz.f32 	%f522, %f294, %f301;
	mul.ftz.f32 	%f302, %f270, %f270;
	fma.rn.ftz.f32 	%f303, %f269, %f269, %f302;
	add.ftz.f32 	%f524, %f297, %f303;
	add.s32 	%r272, %r272, 4;
	add.s32 	%r271, %r271, -4;
	setp.ne.s32 	%p38, %r271, 0;
	@%p38 bra 	$L__BB0_20;

$L__BB0_21:
	setp.eq.s32 	%p39, %r11, 0;
	@%p39 bra 	$L__BB0_25;

	mul.wide.s32 	%rd46, %r272, 4;
	add.s64 	%rd17, %rd15, %rd46;
	ld.global.nc.u32 	%r143, [%rd17];
	add.s64 	%rd18, %rd16, %rd46;
	ld.global.nc.u32 	%r145, [%rd18];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r143;
  cvt.f32.f16 %f304, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r143;
  cvt.f32.f16 %f305, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r145;
  cvt.f32.f16 %f306, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r145;
  cvt.f32.f16 %f307, high;}

	// end inline asm
	mul.ftz.f32 	%f308, %f305, %f307;
	fma.rn.ftz.f32 	%f309, %f304, %f306, %f308;
	add.ftz.f32 	%f523, %f523, %f309;
	mul.ftz.f32 	%f310, %f305, %f305;
	fma.rn.ftz.f32 	%f311, %f304, %f304, %f310;
	add.ftz.f32 	%f522, %f522, %f311;
	mul.ftz.f32 	%f312, %f307, %f307;
	fma.rn.ftz.f32 	%f313, %f306, %f306, %f312;
	add.ftz.f32 	%f524, %f524, %f313;
	setp.eq.s32 	%p40, %r11, 1;
	@%p40 bra 	$L__BB0_25;

	setp.eq.s32 	%p41, %r11, 2;
	ld.global.nc.u32 	%r147, [%rd17+4];
	ld.global.nc.u32 	%r149, [%rd18+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r147;
  cvt.f32.f16 %f314, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r147;
  cvt.f32.f16 %f315, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r149;
  cvt.f32.f16 %f316, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r149;
  cvt.f32.f16 %f317, high;}

	// end inline asm
	mul.ftz.f32 	%f318, %f315, %f317;
	fma.rn.ftz.f32 	%f319, %f314, %f316, %f318;
	add.ftz.f32 	%f523, %f523, %f319;
	mul.ftz.f32 	%f320, %f315, %f315;
	fma.rn.ftz.f32 	%f321, %f314, %f314, %f320;
	add.ftz.f32 	%f522, %f522, %f321;
	mul.ftz.f32 	%f322, %f317, %f317;
	fma.rn.ftz.f32 	%f323, %f316, %f316, %f322;
	add.ftz.f32 	%f524, %f524, %f323;
	@%p41 bra 	$L__BB0_25;

	ld.global.nc.u32 	%r151, [%rd17+8];
	ld.global.nc.u32 	%r153, [%rd18+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r151;
  cvt.f32.f16 %f324, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r151;
  cvt.f32.f16 %f325, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r153;
  cvt.f32.f16 %f326, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r153;
  cvt.f32.f16 %f327, high;}

	// end inline asm
	mul.ftz.f32 	%f328, %f325, %f327;
	fma.rn.ftz.f32 	%f329, %f324, %f326, %f328;
	add.ftz.f32 	%f523, %f523, %f329;
	mul.ftz.f32 	%f330, %f325, %f325;
	fma.rn.ftz.f32 	%f331, %f324, %f324, %f330;
	add.ftz.f32 	%f522, %f522, %f331;
	mul.ftz.f32 	%f332, %f327, %f327;
	fma.rn.ftz.f32 	%f333, %f326, %f326, %f332;
	add.ftz.f32 	%f524, %f524, %f333;

$L__BB0_25:
	setp.eq.s32 	%p42, %r4, 0;
	@%p42 bra 	$L__BB0_27;

	add.s64 	%rd48, %rd15, %rd47;
	ld.global.nc.u16 	%rs3, [%rd48];
	// begin inline asm
	{  cvt.f32.f16 %f334, %rs3;}

	// end inline asm
	add.s64 	%rd49, %rd16, %rd47;
	ld.global.nc.u16 	%rs4, [%rd49];
	// begin inline asm
	{  cvt.f32.f16 %f335, %rs4;}

	// end inline asm
	fma.rn.ftz.f32 	%f523, %f334, %f335, %f523;
	fma.rn.ftz.f32 	%f522, %f334, %f334, %f522;
	fma.rn.ftz.f32 	%f524, %f335, %f335, %f524;

$L__BB0_27:
	mov.b32 	%r155, %f523;
	mov.u32 	%r156, 2;
	mov.u32 	%r157, 31;
	mov.u32 	%r158, 16;
	mov.u32 	%r159, -1;
	shfl.sync.down.b32 	%r160|%p43, %r155, %r158, %r157, %r159;
	mov.b32 	%f337, %r160;
	add.ftz.f32 	%f338, %f523, %f337;
	mov.b32 	%r161, %f522;
	shfl.sync.down.b32 	%r162|%p44, %r161, %r158, %r157, %r159;
	mov.b32 	%f339, %r162;
	add.ftz.f32 	%f340, %f522, %f339;
	mov.b32 	%r163, %f524;
	shfl.sync.down.b32 	%r164|%p45, %r163, %r158, %r157, %r159;
	mov.b32 	%f341, %r164;
	add.ftz.f32 	%f342, %f524, %f341;
	mov.b32 	%r165, %f338;
	mov.u32 	%r166, 8;
	shfl.sync.down.b32 	%r167|%p46, %r165, %r166, %r157, %r159;
	mov.b32 	%f343, %r167;
	add.ftz.f32 	%f344, %f338, %f343;
	mov.b32 	%r168, %f340;
	shfl.sync.down.b32 	%r169|%p47, %r168, %r166, %r157, %r159;
	mov.b32 	%f345, %r169;
	add.ftz.f32 	%f346, %f340, %f345;
	mov.b32 	%r170, %f342;
	shfl.sync.down.b32 	%r171|%p48, %r170, %r166, %r157, %r159;
	mov.b32 	%f347, %r171;
	add.ftz.f32 	%f348, %f342, %f347;
	mov.b32 	%r172, %f344;
	mov.u32 	%r173, 4;
	shfl.sync.down.b32 	%r174|%p49, %r172, %r173, %r157, %r159;
	mov.b32 	%f349, %r174;
	add.ftz.f32 	%f350, %f344, %f349;
	mov.b32 	%r175, %f346;
	shfl.sync.down.b32 	%r176|%p50, %r175, %r173, %r157, %r159;
	mov.b32 	%f351, %r176;
	add.ftz.f32 	%f352, %f346, %f351;
	mov.b32 	%r177, %f348;
	shfl.sync.down.b32 	%r178|%p51, %r177, %r173, %r157, %r159;
	mov.b32 	%f353, %r178;
	add.ftz.f32 	%f354, %f348, %f353;
	mov.b32 	%r179, %f350;
	shfl.sync.down.b32 	%r180|%p52, %r179, %r156, %r157, %r159;
	mov.b32 	%f355, %r180;
	add.ftz.f32 	%f356, %f350, %f355;
	mov.b32 	%r181, %f352;
	shfl.sync.down.b32 	%r182|%p53, %r181, %r156, %r157, %r159;
	mov.b32 	%f357, %r182;
	add.ftz.f32 	%f358, %f352, %f357;
	mov.b32 	%r183, %f354;
	shfl.sync.down.b32 	%r184|%p54, %r183, %r156, %r157, %r159;
	mov.b32 	%f359, %r184;
	add.ftz.f32 	%f360, %f354, %f359;
	mov.b32 	%r185, %f356;
	mov.u32 	%r186, 1;
	shfl.sync.down.b32 	%r187|%p55, %r185, %r186, %r157, %r159;
	mov.b32 	%f361, %r187;
	add.ftz.f32 	%f69, %f356, %f361;
	mov.b32 	%r188, %f358;
	shfl.sync.down.b32 	%r189|%p56, %r188, %r186, %r157, %r159;
	mov.b32 	%f362, %r189;
	add.ftz.f32 	%f363, %f358, %f362;
	mov.b32 	%r190, %f360;
	shfl.sync.down.b32 	%r191|%p57, %r190, %r186, %r157, %r159;
	mov.b32 	%f364, %r191;
	add.ftz.f32 	%f365, %f360, %f364;
	sqrt.approx.ftz.f32 	%f366, %f363;
	sqrt.approx.ftz.f32 	%f367, %f365;
	mul.ftz.f32 	%f70, %f366, %f367;
	setp.lt.ftz.f32 	%p58, %f70, 0f358637BD;
	mov.f32 	%f528, 0f00000000;
	@%p58 bra 	$L__BB0_29;

	div.approx.ftz.f32 	%f528, %f69, %f70;

$L__BB0_29:
	ld.param.f32 	%f494, [compute_multimodal_similarity_fp16_t4_param_11];
	fma.rn.ftz.f32 	%f529, %f528, %f494, %f529;
	add.ftz.f32 	%f530, %f530, %f494;

$L__BB0_30:
	@%p59 bra 	$L__BB0_44;

	ld.param.u32 	%r264, [compute_multimodal_similarity_fp16_t4_param_9];
	setp.lt.s32 	%p60, %r264, 2;
	mul.lo.s32 	%r192, %r17, %r264;
	mul.wide.s32 	%rd50, %r192, 2;
	add.s64 	%rd19, %rd1, %rd50;
	mul.lo.s32 	%r193, %r18, %r264;
	mul.wide.s32 	%rd51, %r193, 2;
	add.s64 	%rd20, %rd1, %rd51;
	mov.f32 	%f540, 0f00000000;
	mov.f32 	%f541, %f540;
	mov.f32 	%f542, %f540;
	@%p60 bra 	$L__BB0_39;

	setp.lt.u32 	%p61, %r8, 3;
	mov.f32 	%f542, 0f00000000;
	mov.u32 	%r275, 0;
	mov.f32 	%f541, %f542;
	mov.f32 	%f540, %f542;
	@%p61 bra 	$L__BB0_35;

	mov.f32 	%f542, 0f00000000;
	mov.u32 	%r275, 0;
	mov.u32 	%r274, %r14;

$L__BB0_34:
	.pragma "nounroll";
	mul.wide.s32 	%rd52, %r275, 4;
	add.s64 	%rd53, %rd19, %rd52;
	ld.global.nc.u32 	%r196, [%rd53];
	add.s64 	%rd54, %rd20, %rd52;
	ld.global.nc.u32 	%r198, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r196;
  cvt.f32.f16 %f378, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r196;
  cvt.f32.f16 %f379, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r198;
  cvt.f32.f16 %f380, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r198;
  cvt.f32.f16 %f381, high;}

	// end inline asm
	mul.ftz.f32 	%f394, %f379, %f381;
	fma.rn.ftz.f32 	%f395, %f378, %f380, %f394;
	add.ftz.f32 	%f396, %f541, %f395;
	mul.ftz.f32 	%f397, %f379, %f379;
	fma.rn.ftz.f32 	%f398, %f378, %f378, %f397;
	add.ftz.f32 	%f399, %f540, %f398;
	mul.ftz.f32 	%f400, %f381, %f381;
	fma.rn.ftz.f32 	%f401, %f380, %f380, %f400;
	add.ftz.f32 	%f402, %f542, %f401;
	ld.global.nc.u32 	%r200, [%rd53+4];
	ld.global.nc.u32 	%r202, [%rd54+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f382, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f383, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r202;
  cvt.f32.f16 %f384, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r202;
  cvt.f32.f16 %f385, high;}

	// end inline asm
	mul.ftz.f32 	%f403, %f383, %f385;
	fma.rn.ftz.f32 	%f404, %f382, %f384, %f403;
	add.ftz.f32 	%f405, %f396, %f404;
	mul.ftz.f32 	%f406, %f383, %f383;
	fma.rn.ftz.f32 	%f407, %f382, %f382, %f406;
	add.ftz.f32 	%f408, %f399, %f407;
	mul.ftz.f32 	%f409, %f385, %f385;
	fma.rn.ftz.f32 	%f410, %f384, %f384, %f409;
	add.ftz.f32 	%f411, %f402, %f410;
	ld.global.nc.u32 	%r204, [%rd53+8];
	ld.global.nc.u32 	%r206, [%rd54+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r204;
  cvt.f32.f16 %f386, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r204;
  cvt.f32.f16 %f387, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r206;
  cvt.f32.f16 %f388, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r206;
  cvt.f32.f16 %f389, high;}

	// end inline asm
	mul.ftz.f32 	%f412, %f387, %f389;
	fma.rn.ftz.f32 	%f413, %f386, %f388, %f412;
	add.ftz.f32 	%f414, %f405, %f413;
	mul.ftz.f32 	%f415, %f387, %f387;
	fma.rn.ftz.f32 	%f416, %f386, %f386, %f415;
	add.ftz.f32 	%f417, %f408, %f416;
	mul.ftz.f32 	%f418, %f389, %f389;
	fma.rn.ftz.f32 	%f419, %f388, %f388, %f418;
	add.ftz.f32 	%f420, %f411, %f419;
	ld.global.nc.u32 	%r208, [%rd53+12];
	ld.global.nc.u32 	%r210, [%rd54+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r208;
  cvt.f32.f16 %f390, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r208;
  cvt.f32.f16 %f391, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r210;
  cvt.f32.f16 %f392, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r210;
  cvt.f32.f16 %f393, high;}

	// end inline asm
	mul.ftz.f32 	%f421, %f391, %f393;
	fma.rn.ftz.f32 	%f422, %f390, %f392, %f421;
	add.ftz.f32 	%f541, %f414, %f422;
	mul.ftz.f32 	%f423, %f391, %f391;
	fma.rn.ftz.f32 	%f424, %f390, %f390, %f423;
	add.ftz.f32 	%f540, %f417, %f424;
	mul.ftz.f32 	%f425, %f393, %f393;
	fma.rn.ftz.f32 	%f426, %f392, %f392, %f425;
	add.ftz.f32 	%f542, %f420, %f426;
	add.s32 	%r275, %r275, 4;
	add.s32 	%r274, %r274, -4;
	setp.ne.s32 	%p62, %r274, 0;
	@%p62 bra 	$L__BB0_34;

$L__BB0_35:
	setp.eq.s32 	%p63, %r13, 0;
	@%p63 bra 	$L__BB0_39;

	mul.wide.s32 	%rd55, %r275, 4;
	add.s64 	%rd21, %rd19, %rd55;
	ld.global.nc.u32 	%r212, [%rd21];
	add.s64 	%rd22, %rd20, %rd55;
	ld.global.nc.u32 	%r214, [%rd22];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r212;
  cvt.f32.f16 %f427, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r212;
  cvt.f32.f16 %f428, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r214;
  cvt.f32.f16 %f429, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r214;
  cvt.f32.f16 %f430, high;}

	// end inline asm
	mul.ftz.f32 	%f431, %f428, %f430;
	fma.rn.ftz.f32 	%f432, %f427, %f429, %f431;
	add.ftz.f32 	%f541, %f541, %f432;
	mul.ftz.f32 	%f433, %f428, %f428;
	fma.rn.ftz.f32 	%f434, %f427, %f427, %f433;
	add.ftz.f32 	%f540, %f540, %f434;
	mul.ftz.f32 	%f435, %f430, %f430;
	fma.rn.ftz.f32 	%f436, %f429, %f429, %f435;
	add.ftz.f32 	%f542, %f542, %f436;
	setp.eq.s32 	%p64, %r13, 1;
	@%p64 bra 	$L__BB0_39;

	setp.eq.s32 	%p65, %r13, 2;
	ld.global.nc.u32 	%r216, [%rd21+4];
	ld.global.nc.u32 	%r218, [%rd22+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r216;
  cvt.f32.f16 %f437, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r216;
  cvt.f32.f16 %f438, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r218;
  cvt.f32.f16 %f439, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r218;
  cvt.f32.f16 %f440, high;}

	// end inline asm
	mul.ftz.f32 	%f441, %f438, %f440;
	fma.rn.ftz.f32 	%f442, %f437, %f439, %f441;
	add.ftz.f32 	%f541, %f541, %f442;
	mul.ftz.f32 	%f443, %f438, %f438;
	fma.rn.ftz.f32 	%f444, %f437, %f437, %f443;
	add.ftz.f32 	%f540, %f540, %f444;
	mul.ftz.f32 	%f445, %f440, %f440;
	fma.rn.ftz.f32 	%f446, %f439, %f439, %f445;
	add.ftz.f32 	%f542, %f542, %f446;
	@%p65 bra 	$L__BB0_39;

	ld.global.nc.u32 	%r220, [%rd21+8];
	ld.global.nc.u32 	%r222, [%rd22+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r220;
  cvt.f32.f16 %f447, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r220;
  cvt.f32.f16 %f448, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r222;
  cvt.f32.f16 %f449, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r222;
  cvt.f32.f16 %f450, high;}

	// end inline asm
	mul.ftz.f32 	%f451, %f448, %f450;
	fma.rn.ftz.f32 	%f452, %f447, %f449, %f451;
	add.ftz.f32 	%f541, %f541, %f452;
	mul.ftz.f32 	%f453, %f448, %f448;
	fma.rn.ftz.f32 	%f454, %f447, %f447, %f453;
	add.ftz.f32 	%f540, %f540, %f454;
	mul.ftz.f32 	%f455, %f450, %f450;
	fma.rn.ftz.f32 	%f456, %f449, %f449, %f455;
	add.ftz.f32 	%f542, %f542, %f456;

$L__BB0_39:
	setp.eq.s32 	%p66, %r5, 0;
	@%p66 bra 	$L__BB0_41;

	add.s64 	%rd57, %rd19, %rd56;
	ld.global.nc.u16 	%rs5, [%rd57];
	// begin inline asm
	{  cvt.f32.f16 %f457, %rs5;}

	// end inline asm
	add.s64 	%rd58, %rd20, %rd56;
	ld.global.nc.u16 	%rs6, [%rd58];
	// begin inline asm
	{  cvt.f32.f16 %f458, %rs6;}

	// end inline asm
	fma.rn.ftz.f32 	%f541, %f457, %f458, %f541;
	fma.rn.ftz.f32 	%f540, %f457, %f457, %f540;
	fma.rn.ftz.f32 	%f542, %f458, %f458, %f542;

$L__BB0_41:
	mov.b32 	%r224, %f541;
	mov.u32 	%r225, 2;
	mov.u32 	%r226, 31;
	mov.u32 	%r227, 16;
	mov.u32 	%r228, -1;
	shfl.sync.down.b32 	%r229|%p67, %r224, %r227, %r226, %r228;
	mov.b32 	%f460, %r229;
	add.ftz.f32 	%f461, %f541, %f460;
	mov.b32 	%r230, %f540;
	shfl.sync.down.b32 	%r231|%p68, %r230, %r227, %r226, %r228;
	mov.b32 	%f462, %r231;
	add.ftz.f32 	%f463, %f540, %f462;
	mov.b32 	%r232, %f542;
	shfl.sync.down.b32 	%r233|%p69, %r232, %r227, %r226, %r228;
	mov.b32 	%f464, %r233;
	add.ftz.f32 	%f465, %f542, %f464;
	mov.b32 	%r234, %f461;
	mov.u32 	%r235, 8;
	shfl.sync.down.b32 	%r236|%p70, %r234, %r235, %r226, %r228;
	mov.b32 	%f466, %r236;
	add.ftz.f32 	%f467, %f461, %f466;
	mov.b32 	%r237, %f463;
	shfl.sync.down.b32 	%r238|%p71, %r237, %r235, %r226, %r228;
	mov.b32 	%f468, %r238;
	add.ftz.f32 	%f469, %f463, %f468;
	mov.b32 	%r239, %f465;
	shfl.sync.down.b32 	%r240|%p72, %r239, %r235, %r226, %r228;
	mov.b32 	%f470, %r240;
	add.ftz.f32 	%f471, %f465, %f470;
	mov.b32 	%r241, %f467;
	mov.u32 	%r242, 4;
	shfl.sync.down.b32 	%r243|%p73, %r241, %r242, %r226, %r228;
	mov.b32 	%f472, %r243;
	add.ftz.f32 	%f473, %f467, %f472;
	mov.b32 	%r244, %f469;
	shfl.sync.down.b32 	%r245|%p74, %r244, %r242, %r226, %r228;
	mov.b32 	%f474, %r245;
	add.ftz.f32 	%f475, %f469, %f474;
	mov.b32 	%r246, %f471;
	shfl.sync.down.b32 	%r247|%p75, %r246, %r242, %r226, %r228;
	mov.b32 	%f476, %r247;
	add.ftz.f32 	%f477, %f471, %f476;
	mov.b32 	%r248, %f473;
	shfl.sync.down.b32 	%r249|%p76, %r248, %r225, %r226, %r228;
	mov.b32 	%f478, %r249;
	add.ftz.f32 	%f479, %f473, %f478;
	mov.b32 	%r250, %f475;
	shfl.sync.down.b32 	%r251|%p77, %r250, %r225, %r226, %r228;
	mov.b32 	%f480, %r251;
	add.ftz.f32 	%f481, %f475, %f480;
	mov.b32 	%r252, %f477;
	shfl.sync.down.b32 	%r253|%p78, %r252, %r225, %r226, %r228;
	mov.b32 	%f482, %r253;
	add.ftz.f32 	%f483, %f477, %f482;
	mov.b32 	%r254, %f479;
	mov.u32 	%r255, 1;
	shfl.sync.down.b32 	%r256|%p79, %r254, %r255, %r226, %r228;
	mov.b32 	%f484, %r256;
	add.ftz.f32 	%f107, %f479, %f484;
	mov.b32 	%r257, %f481;
	shfl.sync.down.b32 	%r258|%p80, %r257, %r255, %r226, %r228;
	mov.b32 	%f485, %r258;
	add.ftz.f32 	%f486, %f481, %f485;
	mov.b32 	%r259, %f483;
	shfl.sync.down.b32 	%r260|%p81, %r259, %r255, %r226, %r228;
	mov.b32 	%f487, %r260;
	add.ftz.f32 	%f488, %f483, %f487;
	sqrt.approx.ftz.f32 	%f489, %f486;
	sqrt.approx.ftz.f32 	%f490, %f488;
	mul.ftz.f32 	%f108, %f489, %f490;
	setp.lt.ftz.f32 	%p82, %f108, 0f358637BD;
	mov.f32 	%f546, 0f00000000;
	@%p82 bra 	$L__BB0_43;

	div.approx.ftz.f32 	%f546, %f107, %f108;

$L__BB0_43:
	ld.param.f32 	%f493, [compute_multimodal_similarity_fp16_t4_param_12];
	fma.rn.ftz.f32 	%f529, %f546, %f493, %f529;
	add.ftz.f32 	%f530, %f530, %f493;

$L__BB0_44:
	setp.leu.ftz.f32 	%p83, %f530, 0f00000000;
	mov.f32 	%f549, 0f00000000;
	@%p83 bra 	$L__BB0_46;

	div.approx.ftz.f32 	%f549, %f529, %f530;

$L__BB0_46:
	ld.param.u32 	%r262, [compute_multimodal_similarity_fp16_t4_param_6];
	shl.b64 	%rd59, %rd10, 2;
	add.s64 	%rd60, %rd9, %rd59;
	st.global.f32 	[%rd60], %f549;
	cvt.u32.u64 	%r261, %rd10;
	add.s32 	%r266, %r261, %r15;
	setp.lt.s32 	%p84, %r266, %r262;
	@%p84 bra 	$L__BB0_2;

$L__BB0_47:
	ret;

}
	// .globl	batch_similarity_streaming_t4
.visible .entry batch_similarity_streaming_t4(
	.param .u64 batch_similarity_streaming_t4_param_0,
	.param .u64 batch_similarity_streaming_t4_param_1,
	.param .u64 batch_similarity_streaming_t4_param_2,
	.param .u32 batch_similarity_streaming_t4_param_3,
	.param .u32 batch_similarity_streaming_t4_param_4,
	.param .u32 batch_similarity_streaming_t4_param_5,
	.param .u32 batch_similarity_streaming_t4_param_6
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<154>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd19, [batch_similarity_streaming_t4_param_0];
	ld.param.u64 	%rd20, [batch_similarity_streaming_t4_param_1];
	ld.param.u64 	%rd21, [batch_similarity_streaming_t4_param_2];
	ld.param.u32 	%r16, [batch_similarity_streaming_t4_param_3];
	ld.param.u32 	%r13, [batch_similarity_streaming_t4_param_4];
	ld.param.u32 	%r14, [batch_similarity_streaming_t4_param_5];
	ld.param.u32 	%r15, [batch_similarity_streaming_t4_param_6];
	mov.u32 	%r17, %ntid.y;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %tid.y;
	mad.lo.s32 	%r1, %r18, %r17, %r19;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r2, %r21, %r20, %r22;
	setp.ge.s32 	%p1, %r1, %r16;
	setp.ge.s32 	%p2, %r2, %r13;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_13;

	cvta.to.global.u64 	%rd1, %rd19;
	mul.lo.s32 	%r23, %r1, %r14;
	cvt.s64.s32 	%rd2, %r23;
	mul.lo.s32 	%r24, %r2, %r14;
	cvt.s64.s32 	%rd3, %r24;
	cvta.to.global.u64 	%rd4, %rd20;
	shr.u32 	%r25, %r14, 31;
	add.s32 	%r26, %r14, %r25;
	shr.s32 	%r3, %r26, 1;
	setp.lt.s32 	%p4, %r14, 2;
	mov.f32 	%f147, 0f00000000;
	mov.f32 	%f148, %f147;
	mov.f32 	%f149, %f147;
	@%p4 bra 	$L__BB1_8;

	add.s32 	%r28, %r3, -1;
	and.b32  	%r94, %r3, 3;
	setp.lt.u32 	%p5, %r28, 3;
	mov.f32 	%f149, 0f00000000;
	mov.u32 	%r93, 0;
	mov.f32 	%f148, %f149;
	mov.f32 	%f147, %f149;
	@%p5 bra 	$L__BB1_5;

	sub.s32 	%r92, %r3, %r94;
	shl.b64 	%rd22, %rd2, 1;
	add.s64 	%rd23, %rd1, %rd22;
	add.s64 	%rd40, %rd23, 8;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	add.s64 	%rd39, %rd25, 8;
	mov.f32 	%f149, 0f00000000;
	mov.u32 	%r93, 0;

$L__BB1_4:
	.pragma "nounroll";
	ld.global.nc.u32 	%r30, [%rd40+-8];
	ld.global.nc.u32 	%r32, [%rd39+-8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	mul.ftz.f32 	%f58, %f43, %f45;
	fma.rn.ftz.f32 	%f59, %f42, %f44, %f58;
	add.ftz.f32 	%f60, %f148, %f59;
	mul.ftz.f32 	%f61, %f43, %f43;
	fma.rn.ftz.f32 	%f62, %f42, %f42, %f61;
	add.ftz.f32 	%f63, %f147, %f62;
	mul.ftz.f32 	%f64, %f45, %f45;
	fma.rn.ftz.f32 	%f65, %f44, %f44, %f64;
	add.ftz.f32 	%f66, %f149, %f65;
	ld.global.nc.u32 	%r34, [%rd40+-4];
	ld.global.nc.u32 	%r36, [%rd39+-4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	mul.ftz.f32 	%f67, %f47, %f49;
	fma.rn.ftz.f32 	%f68, %f46, %f48, %f67;
	add.ftz.f32 	%f69, %f60, %f68;
	mul.ftz.f32 	%f70, %f47, %f47;
	fma.rn.ftz.f32 	%f71, %f46, %f46, %f70;
	add.ftz.f32 	%f72, %f63, %f71;
	mul.ftz.f32 	%f73, %f49, %f49;
	fma.rn.ftz.f32 	%f74, %f48, %f48, %f73;
	add.ftz.f32 	%f75, %f66, %f74;
	ld.global.nc.u32 	%r38, [%rd40];
	ld.global.nc.u32 	%r40, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f52, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f53, high;}

	// end inline asm
	mul.ftz.f32 	%f76, %f51, %f53;
	fma.rn.ftz.f32 	%f77, %f50, %f52, %f76;
	add.ftz.f32 	%f78, %f69, %f77;
	mul.ftz.f32 	%f79, %f51, %f51;
	fma.rn.ftz.f32 	%f80, %f50, %f50, %f79;
	add.ftz.f32 	%f81, %f72, %f80;
	mul.ftz.f32 	%f82, %f53, %f53;
	fma.rn.ftz.f32 	%f83, %f52, %f52, %f82;
	add.ftz.f32 	%f84, %f75, %f83;
	ld.global.nc.u32 	%r42, [%rd40+4];
	ld.global.nc.u32 	%r44, [%rd39+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f54, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f55, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f56, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f57, high;}

	// end inline asm
	mul.ftz.f32 	%f85, %f55, %f57;
	fma.rn.ftz.f32 	%f86, %f54, %f56, %f85;
	add.ftz.f32 	%f148, %f78, %f86;
	mul.ftz.f32 	%f87, %f55, %f55;
	fma.rn.ftz.f32 	%f88, %f54, %f54, %f87;
	add.ftz.f32 	%f147, %f81, %f88;
	mul.ftz.f32 	%f89, %f57, %f57;
	fma.rn.ftz.f32 	%f90, %f56, %f56, %f89;
	add.ftz.f32 	%f149, %f84, %f90;
	add.s32 	%r93, %r93, 4;
	add.s64 	%rd40, %rd40, 16;
	add.s64 	%rd39, %rd39, 16;
	add.s32 	%r92, %r92, -4;
	setp.ne.s32 	%p6, %r92, 0;
	@%p6 bra 	$L__BB1_4;

$L__BB1_5:
	setp.eq.s32 	%p7, %r94, 0;
	@%p7 bra 	$L__BB1_8;

	mul.wide.s32 	%rd26, %r93, 2;
	add.s64 	%rd27, %rd26, %rd3;
	shl.b64 	%rd28, %rd27, 1;
	add.s64 	%rd42, %rd4, %rd28;
	add.s64 	%rd29, %rd26, %rd2;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd41, %rd1, %rd30;

$L__BB1_7:
	.pragma "nounroll";
	ld.global.nc.u32 	%r46, [%rd41];
	ld.global.nc.u32 	%r48, [%rd42];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f91, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f92, high;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f93, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f94, high;}

	// end inline asm
	mul.ftz.f32 	%f95, %f92, %f94;
	fma.rn.ftz.f32 	%f96, %f91, %f93, %f95;
	add.ftz.f32 	%f148, %f148, %f96;
	mul.ftz.f32 	%f97, %f92, %f92;
	fma.rn.ftz.f32 	%f98, %f91, %f91, %f97;
	add.ftz.f32 	%f147, %f147, %f98;
	mul.ftz.f32 	%f99, %f94, %f94;
	fma.rn.ftz.f32 	%f100, %f93, %f93, %f99;
	add.ftz.f32 	%f149, %f149, %f100;
	add.s64 	%rd42, %rd42, 4;
	add.s64 	%rd41, %rd41, 4;
	add.s32 	%r94, %r94, -1;
	setp.ne.s32 	%p8, %r94, 0;
	@%p8 bra 	$L__BB1_7;

$L__BB1_8:
	and.b32  	%r50, %r14, 1;
	setp.eq.b32 	%p9, %r50, 1;
	mov.pred 	%p10, 0;
	xor.pred  	%p11, %p9, %p10;
	not.pred 	%p12, %p11;
	shl.b64 	%rd31, %rd3, 1;
	add.s64 	%rd17, %rd4, %rd31;
	shl.b64 	%rd32, %rd2, 1;
	add.s64 	%rd18, %rd1, %rd32;
	@%p12 bra 	$L__BB1_10;

	add.s32 	%r51, %r14, -1;
	mul.wide.s32 	%rd33, %r51, 2;
	add.s64 	%rd34, %rd18, %rd33;
	ld.global.nc.u16 	%rs1, [%rd34];
	// begin inline asm
	{  cvt.f32.f16 %f101, %rs1;}

	// end inline asm
	add.s64 	%rd35, %rd17, %rd33;
	ld.global.nc.u16 	%rs2, [%rd35];
	// begin inline asm
	{  cvt.f32.f16 %f102, %rs2;}

	// end inline asm
	fma.rn.ftz.f32 	%f148, %f101, %f102, %f148;
	fma.rn.ftz.f32 	%f147, %f101, %f101, %f147;
	fma.rn.ftz.f32 	%f149, %f102, %f102, %f149;

$L__BB1_10:
	mov.b32 	%r52, %f148;
	mov.u32 	%r53, 2;
	mov.u32 	%r54, 31;
	mov.u32 	%r55, 16;
	mov.u32 	%r56, -1;
	shfl.sync.down.b32 	%r57|%p13, %r52, %r55, %r54, %r56;
	mov.b32 	%f104, %r57;
	add.ftz.f32 	%f105, %f148, %f104;
	mov.b32 	%r58, %f147;
	shfl.sync.down.b32 	%r59|%p14, %r58, %r55, %r54, %r56;
	mov.b32 	%f106, %r59;
	add.ftz.f32 	%f107, %f147, %f106;
	mov.b32 	%r60, %f149;
	shfl.sync.down.b32 	%r61|%p15, %r60, %r55, %r54, %r56;
	mov.b32 	%f108, %r61;
	add.ftz.f32 	%f109, %f149, %f108;
	mov.b32 	%r62, %f105;
	mov.u32 	%r63, 8;
	shfl.sync.down.b32 	%r64|%p16, %r62, %r63, %r54, %r56;
	mov.b32 	%f110, %r64;
	add.ftz.f32 	%f111, %f105, %f110;
	mov.b32 	%r65, %f107;
	shfl.sync.down.b32 	%r66|%p17, %r65, %r63, %r54, %r56;
	mov.b32 	%f112, %r66;
	add.ftz.f32 	%f113, %f107, %f112;
	mov.b32 	%r67, %f109;
	shfl.sync.down.b32 	%r68|%p18, %r67, %r63, %r54, %r56;
	mov.b32 	%f114, %r68;
	add.ftz.f32 	%f115, %f109, %f114;
	mov.b32 	%r69, %f111;
	mov.u32 	%r70, 4;
	shfl.sync.down.b32 	%r71|%p19, %r69, %r70, %r54, %r56;
	mov.b32 	%f116, %r71;
	add.ftz.f32 	%f117, %f111, %f116;
	mov.b32 	%r72, %f113;
	shfl.sync.down.b32 	%r73|%p20, %r72, %r70, %r54, %r56;
	mov.b32 	%f118, %r73;
	add.ftz.f32 	%f119, %f113, %f118;
	mov.b32 	%r74, %f115;
	shfl.sync.down.b32 	%r75|%p21, %r74, %r70, %r54, %r56;
	mov.b32 	%f120, %r75;
	add.ftz.f32 	%f121, %f115, %f120;
	mov.b32 	%r76, %f117;
	shfl.sync.down.b32 	%r77|%p22, %r76, %r53, %r54, %r56;
	mov.b32 	%f122, %r77;
	add.ftz.f32 	%f123, %f117, %f122;
	mov.b32 	%r78, %f119;
	shfl.sync.down.b32 	%r79|%p23, %r78, %r53, %r54, %r56;
	mov.b32 	%f124, %r79;
	add.ftz.f32 	%f125, %f119, %f124;
	mov.b32 	%r80, %f121;
	shfl.sync.down.b32 	%r81|%p24, %r80, %r53, %r54, %r56;
	mov.b32 	%f126, %r81;
	add.ftz.f32 	%f127, %f121, %f126;
	mov.b32 	%r82, %f123;
	mov.u32 	%r83, 1;
	shfl.sync.down.b32 	%r84|%p25, %r82, %r83, %r54, %r56;
	mov.b32 	%f128, %r84;
	add.ftz.f32 	%f28, %f123, %f128;
	mov.b32 	%r85, %f125;
	shfl.sync.down.b32 	%r86|%p26, %r85, %r83, %r54, %r56;
	mov.b32 	%f129, %r86;
	add.ftz.f32 	%f130, %f125, %f129;
	mov.b32 	%r87, %f127;
	shfl.sync.down.b32 	%r88|%p27, %r87, %r83, %r54, %r56;
	mov.b32 	%f131, %r88;
	add.ftz.f32 	%f132, %f127, %f131;
	sqrt.approx.ftz.f32 	%f133, %f130;
	sqrt.approx.ftz.f32 	%f134, %f132;
	mul.ftz.f32 	%f29, %f133, %f134;
	setp.lt.ftz.f32 	%p28, %f29, 0f358637BD;
	mov.f32 	%f153, 0f00000000;
	@%p28 bra 	$L__BB1_12;

	div.approx.ftz.f32 	%f153, %f28, %f29;

$L__BB1_12:
	add.s32 	%r89, %r1, %r15;
	mad.lo.s32 	%r90, %r89, %r13, %r2;
	cvta.to.global.u64 	%rd36, %rd21;
	mul.wide.s32 	%rd37, %r90, 4;
	add.s64 	%rd38, %rd36, %rd37;
	st.global.f32 	[%rd38], %f153;

$L__BB1_13:
	ret;

}
	// .globl	topk_selection_warp_t4
.visible .entry topk_selection_warp_t4(
	.param .u64 topk_selection_warp_t4_param_0,
	.param .u64 topk_selection_warp_t4_param_1,
	.param .u64 topk_selection_warp_t4_param_2,
	.param .u32 topk_selection_warp_t4_param_3,
	.param .u32 topk_selection_warp_t4_param_4
)
{
	.reg .pred 	%p<24>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<15>;
	// demoted variable
	.shared .align 4 .b8 _ZZ22topk_selection_warp_t4E13shared_scores[4096];
	// demoted variable
	.shared .align 4 .b8 _ZZ22topk_selection_warp_t4E14shared_indices[4096];

	ld.param.u64 	%rd5, [topk_selection_warp_t4_param_0];
	ld.param.u64 	%rd6, [topk_selection_warp_t4_param_1];
	ld.param.u64 	%rd7, [topk_selection_warp_t4_param_2];
	ld.param.u32 	%r16, [topk_selection_warp_t4_param_3];
	ld.param.u32 	%r17, [topk_selection_warp_t4_param_4];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r16;
	@%p1 bra 	$L__BB2_3;

	mov.u32 	%r3, %ntid.x;
	mul.lo.s32 	%r18, %r1, %r16;
	cvt.s64.s32 	%rd1, %r18;
	cvta.to.global.u64 	%rd2, %rd5;
	mov.u32 	%r64, %r2;

$L__BB2_2:
	cvt.s64.s32 	%rd8, %r64;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.nc.f32 	%f5, [%rd11];
	shl.b32 	%r19, %r64, 2;
	mov.u32 	%r20, _ZZ22topk_selection_warp_t4E13shared_scores;
	add.s32 	%r21, %r20, %r19;
	st.shared.f32 	[%r21], %f5;
	mov.u32 	%r22, _ZZ22topk_selection_warp_t4E14shared_indices;
	add.s32 	%r23, %r22, %r19;
	st.shared.u32 	[%r23], %r64;
	add.s32 	%r64, %r64, %r3;
	setp.lt.s32 	%p2, %r64, %r16;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	bar.sync 	0;
	setp.lt.s32 	%p3, %r17, 1;
	@%p3 bra 	$L__BB2_11;

	mov.u32 	%r6, %ntid.x;
	mul.lo.s32 	%r7, %r1, %r17;
	cvta.to.global.u64 	%rd3, %rd7;
	cvta.to.global.u64 	%rd4, %rd6;
	mov.u32 	%r65, 0;

$L__BB2_5:
	mov.u32 	%r25, -1;
	mov.f32 	%f19, 0fFF7FFFFF;
	mov.u32 	%r68, %r25;
	@%p1 bra 	$L__BB2_8;

	mov.f32 	%f19, 0fFF7FFFFF;
	mov.u32 	%r68, -1;
	mov.u32 	%r66, %r2;

$L__BB2_7:
	shl.b32 	%r27, %r66, 2;
	mov.u32 	%r28, _ZZ22topk_selection_warp_t4E13shared_scores;
	add.s32 	%r29, %r28, %r27;
	ld.shared.f32 	%f8, [%r29];
	setp.gt.ftz.f32 	%p5, %f8, %f19;
	selp.f32 	%f19, %f8, %f19, %p5;
	selp.b32 	%r68, %r66, %r68, %p5;
	add.s32 	%r66, %r66, %r6;
	setp.lt.s32 	%p6, %r66, %r16;
	@%p6 bra 	$L__BB2_7;

$L__BB2_8:
	mov.b32 	%r30, %f19;
	mov.u32 	%r31, 2;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	shfl.sync.down.b32 	%r35|%p7, %r30, %r33, %r32, %r25;
	mov.b32 	%f9, %r35;
	shfl.sync.down.b32 	%r36|%p8, %r68, %r33, %r32, %r25;
	setp.lt.ftz.f32 	%p9, %f19, %f9;
	selp.f32 	%f10, %f9, %f19, %p9;
	selp.b32 	%r37, %r36, %r68, %p9;
	mov.b32 	%r38, %f10;
	mov.u32 	%r39, 8;
	shfl.sync.down.b32 	%r40|%p10, %r38, %r39, %r32, %r25;
	mov.b32 	%f11, %r40;
	shfl.sync.down.b32 	%r41|%p11, %r37, %r39, %r32, %r25;
	setp.lt.ftz.f32 	%p12, %f10, %f11;
	selp.f32 	%f12, %f11, %f10, %p12;
	selp.b32 	%r42, %r41, %r37, %p12;
	mov.b32 	%r43, %f12;
	mov.u32 	%r44, 4;
	shfl.sync.down.b32 	%r45|%p13, %r43, %r44, %r32, %r25;
	mov.b32 	%f13, %r45;
	shfl.sync.down.b32 	%r46|%p14, %r42, %r44, %r32, %r25;
	setp.lt.ftz.f32 	%p15, %f12, %f13;
	selp.f32 	%f14, %f13, %f12, %p15;
	selp.b32 	%r47, %r46, %r42, %p15;
	mov.b32 	%r48, %f14;
	shfl.sync.down.b32 	%r49|%p16, %r48, %r31, %r32, %r25;
	mov.b32 	%f15, %r49;
	shfl.sync.down.b32 	%r50|%p17, %r47, %r31, %r32, %r25;
	setp.lt.ftz.f32 	%p18, %f14, %f15;
	selp.f32 	%f16, %f15, %f14, %p18;
	selp.b32 	%r51, %r50, %r47, %p18;
	mov.b32 	%r52, %f16;
	mov.u32 	%r53, 1;
	shfl.sync.down.b32 	%r54|%p19, %r52, %r53, %r32, %r25;
	mov.b32 	%f17, %r54;
	shfl.sync.down.b32 	%r55|%p20, %r51, %r53, %r32, %r25;
	setp.lt.ftz.f32 	%p21, %f16, %f17;
	selp.f32 	%f4, %f17, %f16, %p21;
	selp.b32 	%r14, %r55, %r51, %p21;
	setp.ne.s32 	%p22, %r2, 0;
	@%p22 bra 	$L__BB2_10;

	shl.b32 	%r56, %r14, 2;
	mov.u32 	%r57, _ZZ22topk_selection_warp_t4E14shared_indices;
	add.s32 	%r58, %r57, %r56;
	ld.shared.u32 	%r59, [%r58];
	add.s32 	%r60, %r65, %r7;
	mul.wide.s32 	%rd12, %r60, 4;
	add.s64 	%rd13, %rd4, %rd12;
	st.global.u32 	[%rd13], %r59;
	add.s64 	%rd14, %rd3, %rd12;
	st.global.f32 	[%rd14], %f4;
	mov.u32 	%r61, _ZZ22topk_selection_warp_t4E13shared_scores;
	add.s32 	%r62, %r61, %r56;
	mov.u32 	%r63, -8388609;
	st.shared.u32 	[%r62], %r63;

$L__BB2_10:
	bar.sync 	0;
	add.s32 	%r65, %r65, 1;
	setp.lt.s32 	%p23, %r65, %r17;
	@%p23 bra 	$L__BB2_5;

$L__BB2_11:
	ret;

}
	// .globl	async_copy_to_peer_t4
.visible .entry async_copy_to_peer_t4(
	.param .u64 async_copy_to_peer_t4_param_0,
	.param .u64 async_copy_to_peer_t4_param_1,
	.param .u32 async_copy_to_peer_t4_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd11, [async_copy_to_peer_t4_param_0];
	ld.param.u64 	%rd12, [async_copy_to_peer_t4_param_1];
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd12;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r35, %r14, %r13, %r15;
	mov.u32 	%r16, %nctaid.x;
	mul.lo.s32 	%r2, %r13, %r16;
	ld.param.u32 	%r17, [async_copy_to_peer_t4_param_2];
	shr.u32 	%r18, %r17, 31;
	add.s32 	%r19, %r17, %r18;
	shr.s32 	%r3, %r19, 1;
	setp.ge.s32 	%p1, %r35, %r3;
	@%p1 bra 	$L__BB3_7;

	add.s32 	%r20, %r3, %r2;
	add.s32 	%r21, %r35, %r2;
	not.b32 	%r22, %r21;
	add.s32 	%r23, %r20, %r22;
	div.u32 	%r4, %r23, %r2;
	add.s32 	%r24, %r4, 1;
	and.b32  	%r34, %r24, 3;
	setp.eq.s32 	%p2, %r34, 0;
	@%p2 bra 	$L__BB3_4;

	mul.wide.s32 	%rd13, %r35, 2;
	shl.b64 	%rd14, %rd13, 1;
	add.s64 	%rd25, %rd2, %rd14;
	mul.wide.s32 	%rd4, %r2, 4;
	add.s64 	%rd24, %rd1, %rd14;

$L__BB3_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r25, [%rd25];
	st.global.u32 	[%rd24], %r25;
	add.s32 	%r35, %r35, %r2;
	add.s64 	%rd25, %rd25, %rd4;
	add.s64 	%rd24, %rd24, %rd4;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB3_3;

$L__BB3_4:
	setp.lt.u32 	%p4, %r4, 3;
	@%p4 bra 	$L__BB3_7;

	mul.wide.s32 	%rd10, %r2, 4;

$L__BB3_6:
	mul.wide.s32 	%rd15, %r35, 4;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd17, %rd2, %rd15;
	ld.global.nc.u32 	%r26, [%rd17];
	st.global.u32 	[%rd16], %r26;
	add.s64 	%rd18, %rd17, %rd10;
	ld.global.nc.u32 	%r27, [%rd18];
	add.s64 	%rd19, %rd16, %rd10;
	st.global.u32 	[%rd19], %r27;
	add.s32 	%r28, %r35, %r2;
	add.s32 	%r29, %r28, %r2;
	add.s64 	%rd20, %rd18, %rd10;
	ld.global.nc.u32 	%r30, [%rd20];
	add.s64 	%rd21, %rd19, %rd10;
	st.global.u32 	[%rd21], %r30;
	add.s32 	%r31, %r29, %r2;
	add.s64 	%rd22, %rd20, %rd10;
	ld.global.nc.u32 	%r32, [%rd22];
	add.s64 	%rd23, %rd21, %rd10;
	st.global.u32 	[%rd23], %r32;
	add.s32 	%r35, %r31, %r2;
	setp.lt.s32 	%p5, %r35, %r3;
	@%p5 bra 	$L__BB3_6;

$L__BB3_7:
	ret;

}

