//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_75
.address_size 64

	// .globl	batch_dot_product_tensor_cores
// _ZZ30batch_dot_product_tensor_coresE6tile_a has been demoted
// _ZZ30batch_dot_product_tensor_coresE6tile_b has been demoted
// _ZZ36batch_cosine_similarity_tensor_coresE10shared_src has been demoted
// _ZZ36batch_cosine_similarity_tensor_coresE10shared_tgt has been demoted
// _ZZ36batch_cosine_similarity_tensor_coresE11shared_dots has been demoted
.extern .shared .align 16 .b8 shared_mem[];

.visible .entry batch_dot_product_tensor_cores(
	.param .u64 batch_dot_product_tensor_cores_param_0,
	.param .u64 batch_dot_product_tensor_cores_param_1,
	.param .u64 batch_dot_product_tensor_cores_param_2,
	.param .u64 batch_dot_product_tensor_cores_param_3,
	.param .u32 batch_dot_product_tensor_cores_param_4,
	.param .u32 batch_dot_product_tensor_cores_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<81>;
	.reg .b32 	%r<70>;
	.reg .b64 	%rd<26>;
	// demoted variable
	.shared .align 2 .b8 _ZZ30batch_dot_product_tensor_coresE6tile_a[512];
	// demoted variable
	.shared .align 2 .b8 _ZZ30batch_dot_product_tensor_coresE6tile_b[512];

	ld.param.u64 	%rd7, [batch_dot_product_tensor_cores_param_0];
	ld.param.u64 	%rd8, [batch_dot_product_tensor_cores_param_1];
	ld.param.u64 	%rd9, [batch_dot_product_tensor_cores_param_2];
	ld.param.u64 	%rd10, [batch_dot_product_tensor_cores_param_3];
	ld.param.u32 	%r21, [batch_dot_product_tensor_cores_param_4];
	ld.param.u32 	%r20, [batch_dot_product_tensor_cores_param_5];
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r24, %r23, %r22, %r1;
	shr.u32 	%r2, %r24, 5;
	and.b32  	%r3, %r1, 31;
	setp.ge.s32 	%p1, %r2, %r21;
	@%p1 bra 	$L__BB0_12;

	cvt.u64.u32 	%rd1, %r2;
	setp.lt.s32 	%p2, %r20, 1;
	mov.f32 	%f57, 0f00000000;
	mov.f32 	%f58, %f57;
	mov.f32 	%f59, %f57;
	mov.f32 	%f60, %f57;
	mov.f32 	%f61, %f57;
	mov.f32 	%f62, %f57;
	mov.f32 	%f63, %f57;
	mov.f32 	%f64, %f57;
	@%p2 bra 	$L__BB0_10;

	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd11, %rd8;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.nc.u32 	%r26, [%rd13];
	cvta.to.global.u64 	%rd14, %rd9;
	add.s64 	%rd15, %rd14, %rd12;
	ld.global.nc.u32 	%r27, [%rd15];
	and.b32  	%r4, %r1, 15;
	or.b32  	%r5, %r3, -32;
	shr.u32 	%r28, %r3, 4;
	shl.b32 	%r29, %r1, 5;
	and.b32  	%r30, %r29, 480;
	mov.u32 	%r31, _ZZ30batch_dot_product_tensor_coresE6tile_b;
	add.s32 	%r32, %r31, %r30;
	shl.b32 	%r33, %r28, 1;
	add.s32 	%r6, %r32, %r33;
	shl.b32 	%r34, %r28, 5;
	mov.u32 	%r35, _ZZ30batch_dot_product_tensor_coresE6tile_a;
	add.s32 	%r36, %r35, %r34;
	shl.b32 	%r37, %r1, 1;
	and.b32  	%r38, %r37, 30;
	add.s32 	%r7, %r36, %r38;
	add.s32 	%r39, %r20, 15;
	shr.s32 	%r40, %r39, 31;
	shr.u32 	%r41, %r40, 28;
	add.s32 	%r42, %r39, %r41;
	shr.s32 	%r8, %r42, 4;
	mul.lo.s32 	%r43, %r27, %r20;
	cvt.s64.s32 	%rd3, %r43;
	mul.lo.s32 	%r44, %r26, %r20;
	cvt.s64.s32 	%rd4, %r44;
	mov.u32 	%r66, 0;
	mov.f32 	%f57, 0f00000000;

$L__BB0_3:
	shl.b32 	%r45, %r66, 4;
	or.b32  	%r12, %r4, %r45;
	cvt.s64.s32 	%rd16, %r12;
	add.s64 	%rd17, %rd16, %rd4;
	shl.b64 	%rd18, %rd17, 1;
	add.s64 	%rd5, %rd2, %rd18;
	add.s64 	%rd19, %rd16, %rd3;
	shl.b64 	%rd20, %rd19, 1;
	add.s64 	%rd6, %rd2, %rd20;
	mov.u32 	%r67, %r7;
	mov.u32 	%r68, %r6;
	mov.u32 	%r69, %r5;

$L__BB0_4:
	add.s32 	%r46, %r69, 47;
	setp.gt.u32 	%p3, %r46, 30;
	setp.ge.s32 	%p4, %r12, %r20;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_6;

	ld.global.nc.u16 	%rs1, [%rd5];
	st.shared.u16 	[%r67], %rs1;
	ld.global.nc.u16 	%rs2, [%rd6];
	st.shared.u16 	[%r68], %rs2;

$L__BB0_6:
	add.s32 	%r69, %r69, 128;
	add.s32 	%r68, %r68, 16;
	add.s32 	%r67, %r67, 256;
	setp.lt.u32 	%p6, %r69, 224;
	@%p6 bra 	$L__BB0_4;

	setp.gt.u32 	%p7, %r3, 15;
	bar.warp.sync 	-1;
	@%p7 bra 	$L__BB0_9;

	mov.u32 	%r47, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r49, %r50, %r51, %r52, %r53, %r54, %r55, %r56}, [%r35], %r47;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r58, %r59, %r60, %r61, %r62, %r63, %r64, %r65}, [%r31], %r47;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {%f64, %f63, %f62, %f61, %f60, %f59, %f58, %f57}, {%r49, %r50, %r51, %r52, %r53, %r54, %r55, %r56}, {%r58, %r59, %r60, %r61, %r62, %r63, %r64, %r65}, {%f64, %f63, %f62, %f61, %f60, %f59, %f58, %f57};

$L__BB0_9:
	add.s32 	%r66, %r66, 1;
	setp.lt.s32 	%p8, %r66, %r8;
	@%p8 bra 	$L__BB0_3;

$L__BB0_10:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB0_12;

	add.ftz.f32 	%f49, %f64, 0f00000000;
	add.ftz.f32 	%f50, %f49, %f63;
	add.ftz.f32 	%f51, %f50, %f62;
	add.ftz.f32 	%f52, %f51, %f61;
	add.ftz.f32 	%f53, %f52, %f60;
	add.ftz.f32 	%f54, %f53, %f59;
	add.ftz.f32 	%f55, %f54, %f58;
	add.ftz.f32 	%f56, %f55, %f57;
	cvta.to.global.u64 	%rd23, %rd10;
	shl.b64 	%rd24, %rd1, 2;
	add.s64 	%rd25, %rd23, %rd24;
	st.global.f32 	[%rd25], %f56;

$L__BB0_12:
	ret;

}
	// .globl	batch_cosine_similarity_tensor_cores
.visible .entry batch_cosine_similarity_tensor_cores(
	.param .u64 batch_cosine_similarity_tensor_cores_param_0,
	.param .u64 batch_cosine_similarity_tensor_cores_param_1,
	.param .u64 batch_cosine_similarity_tensor_cores_param_2,
	.param .u64 batch_cosine_similarity_tensor_cores_param_3,
	.param .u64 batch_cosine_similarity_tensor_cores_param_4,
	.param .u32 batch_cosine_similarity_tensor_cores_param_5,
	.param .u32 batch_cosine_similarity_tensor_cores_param_6
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<28>;
	// demoted variable
	.shared .align 2 .b8 _ZZ36batch_cosine_similarity_tensor_coresE10shared_src[32768];
	// demoted variable
	.shared .align 2 .b8 _ZZ36batch_cosine_similarity_tensor_coresE10shared_tgt[32768];
	// demoted variable
	.shared .align 4 .b8 _ZZ36batch_cosine_similarity_tensor_coresE11shared_dots[64];

	ld.param.u64 	%rd5, [batch_cosine_similarity_tensor_cores_param_0];
	ld.param.u64 	%rd6, [batch_cosine_similarity_tensor_cores_param_1];
	ld.param.u64 	%rd7, [batch_cosine_similarity_tensor_cores_param_2];
	ld.param.u64 	%rd8, [batch_cosine_similarity_tensor_cores_param_3];
	ld.param.u64 	%rd9, [batch_cosine_similarity_tensor_cores_param_4];
	ld.param.u32 	%r15, [batch_cosine_similarity_tensor_cores_param_5];
	ld.param.u32 	%r14, [batch_cosine_similarity_tensor_cores_param_6];
	mov.u32 	%r16, %ctaid.x;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r1, %tid.x;
	shr.u32 	%r2, %r1, 5;
	add.s32 	%r3, %r17, %r2;
	setp.ge.s32 	%p1, %r3, %r15;
	@%p1 bra 	$L__BB1_19;

	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd7;
	mul.wide.s32 	%rd12, %r3, 4;
	add.s64 	%rd13, %rd11, %rd12;
	cvta.to.global.u64 	%rd14, %rd8;
	add.s64 	%rd15, %rd14, %rd12;
	ld.global.nc.u32 	%r4, [%rd13];
	mul.wide.s32 	%rd16, %r4, 4;
	add.s64 	%rd17, %rd10, %rd16;
	ld.global.nc.u32 	%r5, [%rd15];
	mul.wide.s32 	%rd18, %r5, 4;
	add.s64 	%rd19, %rd10, %rd18;
	ld.global.nc.f32 	%f3, [%rd19];
	ld.global.nc.f32 	%f4, [%rd17];
	mul.ftz.f32 	%f1, %f4, %f3;
	setp.lt.ftz.f32 	%p2, %f1, 0f358637BD;
	cvta.to.global.u64 	%rd20, %rd9;
	add.s64 	%rd1, %rd20, %rd12;
	@%p2 bra 	$L__BB1_17;
	bra.uni 	$L__BB1_2;

$L__BB1_17:
	and.b32  	%r58, %r1, 31;
	setp.ne.s32 	%p17, %r58, 0;
	@%p17 bra 	$L__BB1_19;

	mov.u32 	%r59, 0;
	st.global.u32 	[%rd1], %r59;
	bra.uni 	$L__BB1_19;

$L__BB1_2:
	setp.ge.s32 	%p3, %r1, %r14;
	@%p3 bra 	$L__BB1_7;

	cvta.to.global.u64 	%rd2, %rd5;
	mov.u32 	%r6, %ntid.x;
	mul.lo.s32 	%r18, %r5, %r14;
	cvt.s64.s32 	%rd3, %r18;
	mul.lo.s32 	%r19, %r4, %r14;
	cvt.s64.s32 	%rd4, %r19;
	mov.u32 	%r60, %r1;

$L__BB1_4:
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB1_6;

	cvt.s64.s32 	%rd21, %r60;
	add.s64 	%rd22, %rd21, %rd4;
	shl.b64 	%rd23, %rd22, 1;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.u16 	%rs1, [%rd24];
	shl.b32 	%r20, %r2, 11;
	mov.u32 	%r21, _ZZ36batch_cosine_similarity_tensor_coresE10shared_src;
	add.s32 	%r22, %r21, %r20;
	shl.b32 	%r23, %r60, 1;
	add.s32 	%r24, %r22, %r23;
	st.shared.u16 	[%r24], %rs1;
	add.s64 	%rd25, %rd21, %rd3;
	shl.b64 	%rd26, %rd25, 1;
	add.s64 	%rd27, %rd2, %rd26;
	ld.global.nc.u16 	%rs2, [%rd27];
	mov.u32 	%r25, _ZZ36batch_cosine_similarity_tensor_coresE10shared_tgt;
	add.s32 	%r26, %r25, %r20;
	add.s32 	%r27, %r26, %r23;
	st.shared.u16 	[%r27], %rs2;

$L__BB1_6:
	add.s32 	%r60, %r60, %r6;
	setp.lt.s32 	%p5, %r60, %r14;
	@%p5 bra 	$L__BB1_4;

$L__BB1_7:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 511;
	@%p6 bra 	$L__BB1_14;

	setp.lt.s32 	%p7, %r14, 1;
	@%p7 bra 	$L__BB1_12;

	add.s32 	%r30, %r14, 15;
	shr.s32 	%r31, %r30, 31;
	shr.u32 	%r32, %r31, 28;
	add.s32 	%r33, %r30, %r32;
	shr.s32 	%r9, %r33, 4;
	mov.u32 	%r62, 0;
	mov.u32 	%r61, -16;

$L__BB1_10:
	add.s32 	%r62, %r62, 1;
	add.s32 	%r61, %r61, 16;
	setp.lt.s32 	%p8, %r61, %r14;
	@%p8 bra 	$L__BB1_16;

	setp.lt.s32 	%p9, %r62, %r9;
	@%p9 bra 	$L__BB1_10;

$L__BB1_12:
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, 0;
	mov.u32 	%r38, -1;
	shfl.sync.down.b32 	%r39|%p10, %r37, %r36, %r35, %r38;
	mov.b32 	%f5, %r39;
	add.ftz.f32 	%f6, %f5, 0f00000000;
	mov.b32 	%r40, %f6;
	mov.u32 	%r41, 8;
	shfl.sync.down.b32 	%r42|%p11, %r40, %r41, %r35, %r38;
	mov.b32 	%f7, %r42;
	add.ftz.f32 	%f8, %f6, %f7;
	mov.b32 	%r43, %f8;
	mov.u32 	%r44, 4;
	shfl.sync.down.b32 	%r45|%p12, %r43, %r44, %r35, %r38;
	mov.b32 	%f9, %r45;
	add.ftz.f32 	%f10, %f8, %f9;
	mov.b32 	%r46, %f10;
	shfl.sync.down.b32 	%r47|%p13, %r46, %r34, %r35, %r38;
	mov.b32 	%f11, %r47;
	add.ftz.f32 	%f12, %f10, %f11;
	mov.b32 	%r48, %f12;
	mov.u32 	%r49, 1;
	shfl.sync.down.b32 	%r50|%p14, %r48, %r49, %r35, %r38;
	mov.b32 	%f13, %r50;
	add.ftz.f32 	%f2, %f12, %f13;
	and.b32  	%r51, %r1, 31;
	setp.ne.s32 	%p15, %r51, 0;
	@%p15 bra 	$L__BB1_14;

	shl.b32 	%r52, %r2, 2;
	mov.u32 	%r53, _ZZ36batch_cosine_similarity_tensor_coresE11shared_dots;
	add.s32 	%r54, %r53, %r52;
	st.shared.f32 	[%r54], %f2;

$L__BB1_14:
	bar.sync 	0;
	setp.ne.s32 	%p16, %r1, %r2;
	@%p16 bra 	$L__BB1_19;

	shl.b32 	%r55, %r1, 2;
	mov.u32 	%r56, _ZZ36batch_cosine_similarity_tensor_coresE11shared_dots;
	add.s32 	%r57, %r56, %r55;
	ld.shared.f32 	%f14, [%r57];
	div.approx.ftz.f32 	%f15, %f14, %f1;
	st.global.f32 	[%rd1], %f15;

$L__BB1_19:
	ret;

$L__BB1_16:
	trap;

}
	// .globl	compute_multimodal_similarity_tensor_cores
.visible .entry compute_multimodal_similarity_tensor_cores(
	.param .u64 compute_multimodal_similarity_tensor_cores_param_0,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_1,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_2,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_3,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_4,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_5,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_6,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_7,
	.param .u64 compute_multimodal_similarity_tensor_cores_param_8,
	.param .u32 compute_multimodal_similarity_tensor_cores_param_9,
	.param .u32 compute_multimodal_similarity_tensor_cores_param_10,
	.param .u32 compute_multimodal_similarity_tensor_cores_param_11,
	.param .u32 compute_multimodal_similarity_tensor_cores_param_12,
	.param .f32 compute_multimodal_similarity_tensor_cores_param_13,
	.param .f32 compute_multimodal_similarity_tensor_cores_param_14,
	.param .f32 compute_multimodal_similarity_tensor_cores_param_15
)
{
	.reg .pred 	%p<43>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<384>;
	.reg .b32 	%r<270>;
	.reg .b64 	%rd<121>;


	ld.param.u64 	%rd29, [compute_multimodal_similarity_tensor_cores_param_0];
	ld.param.u64 	%rd32, [compute_multimodal_similarity_tensor_cores_param_3];
	ld.param.u32 	%r49, [compute_multimodal_similarity_tensor_cores_param_9];
	ld.param.u32 	%r46, [compute_multimodal_similarity_tensor_cores_param_10];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r50, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r3, %r50, %r1, %r2;
	setp.ge.s32 	%p1, %r3, %r49;
	mov.f32 	%f374, 0f00000000;
	@%p1 bra 	$L__BB2_47;

	mov.f32 	%f375, 0f00000000;
	ld.param.f32 	%f283, [compute_multimodal_similarity_tensor_cores_param_13];
	ld.param.u64 	%rd115, [compute_multimodal_similarity_tensor_cores_param_7];
	ld.param.u64 	%rd114, [compute_multimodal_similarity_tensor_cores_param_6];
	cvta.to.global.u64 	%rd38, %rd114;
	mul.wide.s32 	%rd39, %r3, 4;
	add.s64 	%rd40, %rd38, %rd39;
	ld.global.nc.u32 	%r51, [%rd40];
	cvt.s64.s32 	%rd4, %r51;
	cvta.to.global.u64 	%rd41, %rd115;
	add.s64 	%rd42, %rd41, %rd39;
	ld.global.nc.u32 	%r52, [%rd42];
	cvt.s64.s32 	%rd5, %r52;
	setp.eq.s64 	%p2, %rd29, 0;
	setp.leu.ftz.f32 	%p3, %f283, 0f00000000;
	or.pred  	%p4, %p2, %p3;
	setp.eq.s64 	%p5, %rd32, 0;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB2_24;

	mov.f32 	%f374, 0f00000000;
	mov.f32 	%f375, 0f00000000;
	cvta.to.global.u64 	%rd43, %rd32;
	shl.b64 	%rd44, %rd5, 2;
	add.s64 	%rd45, %rd43, %rd44;
	ld.global.nc.f32 	%f159, [%rd45];
	shl.b64 	%rd46, %rd4, 2;
	add.s64 	%rd47, %rd43, %rd46;
	ld.global.nc.f32 	%f160, [%rd47];
	mul.ftz.f32 	%f1, %f160, %f159;
	setp.leu.ftz.f32 	%p7, %f1, 0f358637BD;
	@%p7 bra 	$L__BB2_24;

	mov.u32 	%r246, %tid.x;
	setp.ge.s32 	%p8, %r246, %r46;
	@%p8 bra 	$L__BB2_6;

	ld.param.u64 	%rd116, [compute_multimodal_similarity_tensor_cores_param_0];
	mov.u32 	%r255, %tid.x;
	cvta.to.global.u64 	%rd6, %rd116;
	cvt.u32.u64 	%r53, %rd4;
	mul.lo.s32 	%r54, %r53, %r46;
	cvt.s64.s32 	%rd7, %r54;
	cvt.u32.u64 	%r55, %rd5;
	mul.lo.s32 	%r56, %r55, %r46;
	cvt.s64.s32 	%rd8, %r56;
	mov.u32 	%r58, shared_mem;

$L__BB2_5:
	cvt.s64.s32 	%rd48, %r255;
	add.s64 	%rd49, %rd48, %rd7;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd6, %rd50;
	ld.global.nc.u16 	%rs1, [%rd51];
	shl.b32 	%r57, %r255, 1;
	add.s32 	%r59, %r58, %r57;
	st.shared.u16 	[%r59], %rs1;
	add.s64 	%rd52, %rd48, %rd8;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd6, %rd53;
	ld.global.nc.u16 	%rs2, [%rd54];
	st.shared.u16 	[%r59+2048], %rs2;
	add.s32 	%r255, %r255, %r1;
	setp.lt.s32 	%p9, %r255, %r46;
	@%p9 bra 	$L__BB2_5;

$L__BB2_6:
	bar.sync 	0;
	setp.lt.s32 	%p10, %r46, 1;
	mov.f32 	%f295, 0f00000000;
	mov.f32 	%f296, %f295;
	mov.f32 	%f297, %f295;
	mov.f32 	%f298, %f295;
	mov.f32 	%f299, %f295;
	mov.f32 	%f300, %f295;
	mov.f32 	%f301, %f295;
	mov.f32 	%f302, %f295;
	@%p10 bra 	$L__BB2_23;

	add.s32 	%r61, %r46, 15;
	shr.s32 	%r62, %r61, 31;
	shr.u32 	%r63, %r62, 28;
	add.s32 	%r64, %r61, %r63;
	shr.s32 	%r65, %r64, 4;
	max.s32 	%r6, %r65, 1;
	add.s32 	%r66, %r6, -1;
	setp.lt.u32 	%p11, %r66, 3;
	mov.u32 	%r258, 0;
	mov.f32 	%f295, 0f00000000;
	@%p11 bra 	$L__BB2_18;

	and.b32  	%r248, %r6, 3;
	sub.s32 	%r257, %r6, %r248;
	mov.u32 	%r258, 0;
	mov.f32 	%f295, 0f00000000;
	mov.f32 	%f296, %f295;
	mov.f32 	%f297, %f295;
	mov.f32 	%f298, %f295;
	mov.f32 	%f299, %f295;
	mov.f32 	%f300, %f295;
	mov.f32 	%f301, %f295;
	mov.f32 	%f302, %f295;

$L__BB2_9:
	shl.b32 	%r11, %r258, 4;
	setp.ge.s32 	%p12, %r11, %r46;
	@%p12 bra 	$L__BB2_11;

	shl.b32 	%r68, %r11, 1;
	mov.u32 	%r69, shared_mem;
	add.s32 	%r70, %r69, %r68;
	mov.u32 	%r71, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r72, %r73, %r74, %r75, %r76, %r77, %r78, %r79}, [%r70], %r71;
	add.s32 	%r80, %r70, 2048;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r81, %r82, %r83, %r84, %r85, %r86, %r87, %r88}, [%r80], %r71;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295}, {%r72, %r73, %r74, %r75, %r76, %r77, %r78, %r79}, {%r81, %r82, %r83, %r84, %r85, %r86, %r87, %r88}, {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295};

$L__BB2_11:
	shl.b32 	%r251, %r258, 4;
	add.s32 	%r12, %r251, 16;
	setp.ge.s32 	%p13, %r12, %r46;
	@%p13 bra 	$L__BB2_13;

	shl.b32 	%r254, %r258, 4;
	shl.b32 	%r89, %r12, 1;
	mov.u32 	%r90, shared_mem;
	add.s32 	%r91, %r90, %r89;
	mov.u32 	%r92, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r93, %r94, %r95, %r96, %r97, %r98, %r99, %r100}, [%r91], %r92;
	shl.b32 	%r101, %r254, 1;
	add.s32 	%r102, %r90, %r101;
	add.s32 	%r103, %r102, 2080;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r104, %r105, %r106, %r107, %r108, %r109, %r110, %r111}, [%r103], %r92;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295}, {%r93, %r94, %r95, %r96, %r97, %r98, %r99, %r100}, {%r104, %r105, %r106, %r107, %r108, %r109, %r110, %r111}, {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295};

$L__BB2_13:
	shl.b32 	%r252, %r258, 4;
	add.s32 	%r13, %r252, 32;
	setp.ge.s32 	%p14, %r13, %r46;
	@%p14 bra 	$L__BB2_15;

	shl.b32 	%r253, %r258, 4;
	shl.b32 	%r112, %r13, 1;
	mov.u32 	%r113, shared_mem;
	add.s32 	%r114, %r113, %r112;
	mov.u32 	%r115, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r116, %r117, %r118, %r119, %r120, %r121, %r122, %r123}, [%r114], %r115;
	shl.b32 	%r124, %r253, 1;
	add.s32 	%r125, %r113, %r124;
	add.s32 	%r126, %r125, 2112;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r127, %r128, %r129, %r130, %r131, %r132, %r133, %r134}, [%r126], %r115;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295}, {%r116, %r117, %r118, %r119, %r120, %r121, %r122, %r123}, {%r127, %r128, %r129, %r130, %r131, %r132, %r133, %r134}, {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295};

$L__BB2_15:
	shl.b32 	%r244, %r258, 4;
	add.s32 	%r14, %r244, 48;
	setp.ge.s32 	%p15, %r14, %r46;
	@%p15 bra 	$L__BB2_17;

	shl.b32 	%r245, %r258, 4;
	shl.b32 	%r135, %r14, 1;
	mov.u32 	%r136, shared_mem;
	add.s32 	%r137, %r136, %r135;
	mov.u32 	%r138, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r139, %r140, %r141, %r142, %r143, %r144, %r145, %r146}, [%r137], %r138;
	shl.b32 	%r147, %r245, 1;
	add.s32 	%r148, %r136, %r147;
	add.s32 	%r149, %r148, 2144;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r150, %r151, %r152, %r153, %r154, %r155, %r156, %r157}, [%r149], %r138;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295}, {%r139, %r140, %r141, %r142, %r143, %r144, %r145, %r146}, {%r150, %r151, %r152, %r153, %r154, %r155, %r156, %r157}, {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295};

$L__BB2_17:
	add.s32 	%r258, %r258, 4;
	add.s32 	%r257, %r257, -4;
	setp.ne.s32 	%p16, %r257, 0;
	@%p16 bra 	$L__BB2_9;

$L__BB2_18:
	and.b32  	%r249, %r6, 3;
	setp.eq.s32 	%p17, %r249, 0;
	@%p17 bra 	$L__BB2_23;

	and.b32  	%r261, %r6, 3;
	shl.b32 	%r259, %r258, 4;
	shl.b32 	%r158, %r258, 5;
	mov.u32 	%r159, shared_mem;
	add.s32 	%r260, %r159, %r158;

$L__BB2_20:
	.pragma "nounroll";
	setp.ge.s32 	%p18, %r259, %r46;
	@%p18 bra 	$L__BB2_22;

	mov.u32 	%r160, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r161, %r162, %r163, %r164, %r165, %r166, %r167, %r168}, [%r260], %r160;
	add.s32 	%r169, %r260, 2048;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r170, %r171, %r172, %r173, %r174, %r175, %r176, %r177}, [%r169], %r160;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295}, {%r161, %r162, %r163, %r164, %r165, %r166, %r167, %r168}, {%r170, %r171, %r172, %r173, %r174, %r175, %r176, %r177}, {%f302, %f301, %f300, %f299, %f298, %f297, %f296, %f295};

$L__BB2_22:
	add.s32 	%r260, %r260, 32;
	add.s32 	%r259, %r259, 16;
	add.s32 	%r261, %r261, -1;
	setp.ne.s32 	%p19, %r261, 0;
	@%p19 bra 	$L__BB2_20;

$L__BB2_23:
	ld.param.f32 	%f278, [compute_multimodal_similarity_tensor_cores_param_13];
	add.ftz.f32 	%f186, %f302, 0f00000000;
	add.ftz.f32 	%f187, %f186, %f301;
	add.ftz.f32 	%f188, %f187, %f300;
	add.ftz.f32 	%f189, %f188, %f299;
	add.ftz.f32 	%f190, %f189, %f298;
	add.ftz.f32 	%f191, %f190, %f297;
	add.ftz.f32 	%f192, %f191, %f296;
	add.ftz.f32 	%f193, %f192, %f295;
	div.approx.ftz.f32 	%f194, %f193, %f1;
	fma.rn.ftz.f32 	%f374, %f194, %f278, 0f00000000;
	add.ftz.f32 	%f375, %f278, 0f00000000;

$L__BB2_24:
	ld.param.u64 	%rd104, [compute_multimodal_similarity_tensor_cores_param_4];
	ld.param.f32 	%f279, [compute_multimodal_similarity_tensor_cores_param_14];
	ld.param.u64 	%rd98, [compute_multimodal_similarity_tensor_cores_param_1];
	setp.eq.s64 	%p20, %rd98, 0;
	setp.leu.ftz.f32 	%p21, %f279, 0f00000000;
	or.pred  	%p22, %p20, %p21;
	setp.eq.s64 	%p23, %rd104, 0;
	or.pred  	%p24, %p23, %p22;
	@%p24 bra 	$L__BB2_34;

	ld.param.u64 	%rd109, [compute_multimodal_similarity_tensor_cores_param_4];
	ld.param.u32 	%r242, [compute_multimodal_similarity_tensor_cores_param_11];
	cvta.to.global.u64 	%rd65, %rd109;
	cvt.u32.u64 	%r178, %rd5;
	cvt.u32.u64 	%r179, %rd4;
	mul.lo.s32 	%r180, %r179, %r242;
	cvt.s64.s32 	%rd9, %r180;
	mul.lo.s32 	%r181, %r178, %r242;
	cvt.s64.s32 	%rd10, %r181;
	shl.b64 	%rd66, %rd5, 2;
	add.s64 	%rd67, %rd65, %rd66;
	ld.global.nc.f32 	%f195, [%rd67];
	shl.b64 	%rd68, %rd4, 2;
	add.s64 	%rd69, %rd65, %rd68;
	ld.global.nc.f32 	%f196, [%rd69];
	mul.ftz.f32 	%f126, %f196, %f195;
	setp.leu.ftz.f32 	%p25, %f126, 0f358637BD;
	@%p25 bra 	$L__BB2_34;

	ld.param.u32 	%r243, [compute_multimodal_similarity_tensor_cores_param_11];
	shr.u32 	%r182, %r243, 31;
	add.s32 	%r183, %r243, %r182;
	shr.s32 	%r26, %r183, 1;
	setp.lt.s32 	%p26, %r243, 2;
	mov.f32 	%f373, 0f00000000;
	@%p26 bra 	$L__BB2_33;

	add.s32 	%r185, %r26, -1;
	and.b32  	%r265, %r26, 3;
	setp.lt.u32 	%p27, %r185, 3;
	mov.f32 	%f373, 0f00000000;
	mov.u32 	%r264, 0;
	@%p27 bra 	$L__BB2_30;

	ld.param.u64 	%rd111, [compute_multimodal_similarity_tensor_cores_param_1];
	cvta.to.global.u64 	%rd110, %rd111;
	sub.s32 	%r263, %r26, %r265;
	shl.b64 	%rd70, %rd10, 1;
	add.s64 	%rd11, %rd110, %rd70;
	shl.b64 	%rd71, %rd9, 1;
	add.s64 	%rd12, %rd110, %rd71;
	mov.f32 	%f373, 0f00000000;
	mov.u32 	%r264, 0;

$L__BB2_29:
	.pragma "nounroll";
	mul.wide.s32 	%rd72, %r264, 4;
	add.s64 	%rd73, %rd12, %rd72;
	ld.global.nc.u32 	%r187, [%rd73];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r187;
  cvt.f32.f16 %f201, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r187;
  cvt.f32.f16 %f202, high;}

	// end inline asm
	add.s64 	%rd74, %rd11, %rd72;
	ld.global.nc.u32 	%r189, [%rd74];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r189;
  cvt.f32.f16 %f203, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r189;
  cvt.f32.f16 %f204, high;}

	// end inline asm
	mul.ftz.f32 	%f217, %f202, %f204;
	fma.rn.ftz.f32 	%f218, %f201, %f203, %f217;
	add.ftz.f32 	%f219, %f373, %f218;
	ld.global.nc.u32 	%r191, [%rd73+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r191;
  cvt.f32.f16 %f205, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r191;
  cvt.f32.f16 %f206, high;}

	// end inline asm
	ld.global.nc.u32 	%r193, [%rd74+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r193;
  cvt.f32.f16 %f207, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r193;
  cvt.f32.f16 %f208, high;}

	// end inline asm
	mul.ftz.f32 	%f220, %f206, %f208;
	fma.rn.ftz.f32 	%f221, %f205, %f207, %f220;
	add.ftz.f32 	%f222, %f219, %f221;
	ld.global.nc.u32 	%r195, [%rd73+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r195;
  cvt.f32.f16 %f209, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r195;
  cvt.f32.f16 %f210, high;}

	// end inline asm
	ld.global.nc.u32 	%r197, [%rd74+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r197;
  cvt.f32.f16 %f211, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r197;
  cvt.f32.f16 %f212, high;}

	// end inline asm
	mul.ftz.f32 	%f223, %f210, %f212;
	fma.rn.ftz.f32 	%f224, %f209, %f211, %f223;
	add.ftz.f32 	%f225, %f222, %f224;
	ld.global.nc.u32 	%r199, [%rd73+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f213, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f214, high;}

	// end inline asm
	ld.global.nc.u32 	%r201, [%rd74+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r201;
  cvt.f32.f16 %f215, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r201;
  cvt.f32.f16 %f216, high;}

	// end inline asm
	mul.ftz.f32 	%f226, %f214, %f216;
	fma.rn.ftz.f32 	%f227, %f213, %f215, %f226;
	add.ftz.f32 	%f373, %f225, %f227;
	add.s32 	%r264, %r264, 4;
	add.s32 	%r263, %r263, -4;
	setp.ne.s32 	%p28, %r263, 0;
	@%p28 bra 	$L__BB2_29;

$L__BB2_30:
	setp.eq.s32 	%p29, %r265, 0;
	@%p29 bra 	$L__BB2_33;

	ld.param.u64 	%rd113, [compute_multimodal_similarity_tensor_cores_param_1];
	cvta.to.global.u64 	%rd112, %rd113;
	mul.wide.s32 	%rd75, %r264, 2;
	add.s64 	%rd76, %rd75, %rd10;
	shl.b64 	%rd77, %rd76, 1;
	add.s64 	%rd118, %rd112, %rd77;
	add.s64 	%rd78, %rd75, %rd9;
	shl.b64 	%rd79, %rd78, 1;
	add.s64 	%rd117, %rd112, %rd79;

$L__BB2_32:
	.pragma "nounroll";
	ld.global.nc.u32 	%r203, [%rd117];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r203;
  cvt.f32.f16 %f228, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r203;
  cvt.f32.f16 %f229, high;}

	// end inline asm
	ld.global.nc.u32 	%r205, [%rd118];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r205;
  cvt.f32.f16 %f230, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r205;
  cvt.f32.f16 %f231, high;}

	// end inline asm
	mul.ftz.f32 	%f232, %f229, %f231;
	fma.rn.ftz.f32 	%f233, %f228, %f230, %f232;
	add.ftz.f32 	%f373, %f373, %f233;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd117, %rd117, 4;
	add.s32 	%r265, %r265, -1;
	setp.ne.s32 	%p30, %r265, 0;
	@%p30 bra 	$L__BB2_32;

$L__BB2_33:
	ld.param.f32 	%f282, [compute_multimodal_similarity_tensor_cores_param_14];
	div.approx.ftz.f32 	%f234, %f373, %f126;
	fma.rn.ftz.f32 	%f374, %f234, %f282, %f374;
	add.ftz.f32 	%f375, %f375, %f282;

$L__BB2_34:
	ld.param.u64 	%rd100, [compute_multimodal_similarity_tensor_cores_param_5];
	ld.param.f32 	%f280, [compute_multimodal_similarity_tensor_cores_param_15];
	ld.param.u64 	%rd99, [compute_multimodal_similarity_tensor_cores_param_2];
	setp.eq.s64 	%p31, %rd99, 0;
	setp.leu.ftz.f32 	%p32, %f280, 0f00000000;
	or.pred  	%p33, %p31, %p32;
	setp.eq.s64 	%p34, %rd100, 0;
	or.pred  	%p35, %p34, %p33;
	@%p35 bra 	$L__BB2_44;

	ld.param.u32 	%r240, [compute_multimodal_similarity_tensor_cores_param_12];
	ld.param.u64 	%rd103, [compute_multimodal_similarity_tensor_cores_param_5];
	cvta.to.global.u64 	%rd80, %rd103;
	cvt.u32.u64 	%r207, %rd5;
	cvt.u32.u64 	%r208, %rd4;
	mul.lo.s32 	%r209, %r208, %r240;
	cvt.s64.s32 	%rd19, %r209;
	mul.lo.s32 	%r210, %r207, %r240;
	cvt.s64.s32 	%rd20, %r210;
	shl.b64 	%rd81, %rd5, 2;
	add.s64 	%rd82, %rd80, %rd81;
	ld.global.nc.f32 	%f235, [%rd82];
	shl.b64 	%rd83, %rd4, 2;
	add.s64 	%rd84, %rd80, %rd83;
	ld.global.nc.f32 	%f236, [%rd84];
	mul.ftz.f32 	%f138, %f236, %f235;
	setp.leu.ftz.f32 	%p36, %f138, 0f358637BD;
	@%p36 bra 	$L__BB2_44;

	ld.param.u32 	%r241, [compute_multimodal_similarity_tensor_cores_param_12];
	shr.u32 	%r211, %r241, 31;
	add.s32 	%r212, %r241, %r211;
	shr.s32 	%r36, %r212, 1;
	setp.lt.s32 	%p37, %r241, 2;
	mov.f32 	%f380, 0f00000000;
	@%p37 bra 	$L__BB2_43;

	add.s32 	%r214, %r36, -1;
	and.b32  	%r269, %r36, 3;
	setp.lt.u32 	%p38, %r214, 3;
	mov.f32 	%f380, 0f00000000;
	mov.u32 	%r268, 0;
	@%p38 bra 	$L__BB2_40;

	ld.param.u64 	%rd106, [compute_multimodal_similarity_tensor_cores_param_2];
	cvta.to.global.u64 	%rd105, %rd106;
	sub.s32 	%r267, %r36, %r269;
	shl.b64 	%rd85, %rd20, 1;
	add.s64 	%rd21, %rd105, %rd85;
	shl.b64 	%rd86, %rd19, 1;
	add.s64 	%rd22, %rd105, %rd86;
	mov.f32 	%f380, 0f00000000;
	mov.u32 	%r268, 0;

$L__BB2_39:
	.pragma "nounroll";
	mul.wide.s32 	%rd87, %r268, 4;
	add.s64 	%rd88, %rd22, %rd87;
	ld.global.nc.u32 	%r216, [%rd88];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r216;
  cvt.f32.f16 %f241, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r216;
  cvt.f32.f16 %f242, high;}

	// end inline asm
	add.s64 	%rd89, %rd21, %rd87;
	ld.global.nc.u32 	%r218, [%rd89];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r218;
  cvt.f32.f16 %f243, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r218;
  cvt.f32.f16 %f244, high;}

	// end inline asm
	mul.ftz.f32 	%f257, %f242, %f244;
	fma.rn.ftz.f32 	%f258, %f241, %f243, %f257;
	add.ftz.f32 	%f259, %f380, %f258;
	ld.global.nc.u32 	%r220, [%rd88+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r220;
  cvt.f32.f16 %f245, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r220;
  cvt.f32.f16 %f246, high;}

	// end inline asm
	ld.global.nc.u32 	%r222, [%rd89+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r222;
  cvt.f32.f16 %f247, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r222;
  cvt.f32.f16 %f248, high;}

	// end inline asm
	mul.ftz.f32 	%f260, %f246, %f248;
	fma.rn.ftz.f32 	%f261, %f245, %f247, %f260;
	add.ftz.f32 	%f262, %f259, %f261;
	ld.global.nc.u32 	%r224, [%rd88+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r224;
  cvt.f32.f16 %f249, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r224;
  cvt.f32.f16 %f250, high;}

	// end inline asm
	ld.global.nc.u32 	%r226, [%rd89+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r226;
  cvt.f32.f16 %f251, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r226;
  cvt.f32.f16 %f252, high;}

	// end inline asm
	mul.ftz.f32 	%f263, %f250, %f252;
	fma.rn.ftz.f32 	%f264, %f249, %f251, %f263;
	add.ftz.f32 	%f265, %f262, %f264;
	ld.global.nc.u32 	%r228, [%rd88+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r228;
  cvt.f32.f16 %f253, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r228;
  cvt.f32.f16 %f254, high;}

	// end inline asm
	ld.global.nc.u32 	%r230, [%rd89+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r230;
  cvt.f32.f16 %f255, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r230;
  cvt.f32.f16 %f256, high;}

	// end inline asm
	mul.ftz.f32 	%f266, %f254, %f256;
	fma.rn.ftz.f32 	%f267, %f253, %f255, %f266;
	add.ftz.f32 	%f380, %f265, %f267;
	add.s32 	%r268, %r268, 4;
	add.s32 	%r267, %r267, -4;
	setp.ne.s32 	%p39, %r267, 0;
	@%p39 bra 	$L__BB2_39;

$L__BB2_40:
	setp.eq.s32 	%p40, %r269, 0;
	@%p40 bra 	$L__BB2_43;

	ld.param.u64 	%rd108, [compute_multimodal_similarity_tensor_cores_param_2];
	cvta.to.global.u64 	%rd107, %rd108;
	mul.wide.s32 	%rd90, %r268, 2;
	add.s64 	%rd91, %rd90, %rd20;
	shl.b64 	%rd92, %rd91, 1;
	add.s64 	%rd120, %rd107, %rd92;
	add.s64 	%rd93, %rd90, %rd19;
	shl.b64 	%rd94, %rd93, 1;
	add.s64 	%rd119, %rd107, %rd94;

$L__BB2_42:
	.pragma "nounroll";
	ld.global.nc.u32 	%r232, [%rd119];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r232;
  cvt.f32.f16 %f268, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r232;
  cvt.f32.f16 %f269, high;}

	// end inline asm
	ld.global.nc.u32 	%r234, [%rd120];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r234;
  cvt.f32.f16 %f270, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r234;
  cvt.f32.f16 %f271, high;}

	// end inline asm
	mul.ftz.f32 	%f272, %f269, %f271;
	fma.rn.ftz.f32 	%f273, %f268, %f270, %f272;
	add.ftz.f32 	%f380, %f380, %f273;
	add.s64 	%rd120, %rd120, 4;
	add.s64 	%rd119, %rd119, 4;
	add.s32 	%r269, %r269, -1;
	setp.ne.s32 	%p41, %r269, 0;
	@%p41 bra 	$L__BB2_42;

$L__BB2_43:
	ld.param.f32 	%f281, [compute_multimodal_similarity_tensor_cores_param_15];
	div.approx.ftz.f32 	%f274, %f380, %f138;
	fma.rn.ftz.f32 	%f374, %f274, %f281, %f374;
	add.ftz.f32 	%f375, %f375, %f281;

$L__BB2_44:
	setp.leu.ftz.f32 	%p42, %f375, 0f00000000;
	mov.f32 	%f383, 0f00000000;
	@%p42 bra 	$L__BB2_46;

	div.approx.ftz.f32 	%f383, %f374, %f375;

$L__BB2_46:
	mov.u32 	%r239, %tid.x;
	mov.u32 	%r238, %ntid.x;
	mov.u32 	%r237, %ctaid.x;
	mad.lo.s32 	%r236, %r237, %r238, %r239;
	cvt.s64.s32 	%rd102, %r236;
	ld.param.u64 	%rd101, [compute_multimodal_similarity_tensor_cores_param_8];
	cvta.to.global.u64 	%rd95, %rd101;
	shl.b64 	%rd96, %rd102, 2;
	add.s64 	%rd97, %rd95, %rd96;
	st.global.f32 	[%rd97], %f383;

$L__BB2_47:
	ret;

}
	// .globl	precompute_norms_kernel
.visible .entry precompute_norms_kernel(
	.param .u64 precompute_norms_kernel_param_0,
	.param .u64 precompute_norms_kernel_param_1,
	.param .u32 precompute_norms_kernel_param_2,
	.param .u32 precompute_norms_kernel_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<65>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd11, [precompute_norms_kernel_param_0];
	ld.param.u64 	%rd10, [precompute_norms_kernel_param_1];
	ld.param.u32 	%r13, [precompute_norms_kernel_param_2];
	ld.param.u32 	%r12, [precompute_norms_kernel_param_3];
	cvta.to.global.u64 	%rd1, %rd11;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r15, %r14, %r16;
	setp.ge.s32 	%p1, %r1, %r13;
	@%p1 bra 	$L__BB3_11;

	mul.lo.s32 	%r17, %r1, %r12;
	cvt.s64.s32 	%rd2, %r17;
	shr.u32 	%r18, %r12, 31;
	add.s32 	%r19, %r12, %r18;
	shr.s32 	%r2, %r19, 1;
	setp.lt.s32 	%p2, %r12, 2;
	mov.f32 	%f63, 0f00000000;
	@%p2 bra 	$L__BB3_8;

	add.s32 	%r21, %r2, -1;
	and.b32  	%r46, %r2, 7;
	setp.lt.u32 	%p3, %r21, 7;
	mov.f32 	%f63, 0f00000000;
	mov.u32 	%r45, 0;
	@%p3 bra 	$L__BB3_5;

	sub.s32 	%r44, %r2, %r46;
	shl.b64 	%rd12, %rd2, 1;
	add.s64 	%rd13, %rd1, %rd12;
	add.s64 	%rd23, %rd13, 16;
	mov.f32 	%f63, 0f00000000;
	mov.u32 	%r45, 0;

$L__BB3_4:
	.pragma "nounroll";
	ld.global.nc.u32 	%r23, [%rd23+-16];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	mul.ftz.f32 	%f30, %f15, %f15;
	fma.rn.ftz.f32 	%f31, %f14, %f14, %f30;
	add.ftz.f32 	%f32, %f63, %f31;
	ld.global.nc.u32 	%r25, [%rd23+-12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	mul.ftz.f32 	%f33, %f17, %f17;
	fma.rn.ftz.f32 	%f34, %f16, %f16, %f33;
	add.ftz.f32 	%f35, %f32, %f34;
	ld.global.nc.u32 	%r27, [%rd23+-8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	mul.ftz.f32 	%f36, %f19, %f19;
	fma.rn.ftz.f32 	%f37, %f18, %f18, %f36;
	add.ftz.f32 	%f38, %f35, %f37;
	ld.global.nc.u32 	%r29, [%rd23+-4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	mul.ftz.f32 	%f39, %f21, %f21;
	fma.rn.ftz.f32 	%f40, %f20, %f20, %f39;
	add.ftz.f32 	%f41, %f38, %f40;
	ld.global.nc.u32 	%r31, [%rd23];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	mul.ftz.f32 	%f42, %f23, %f23;
	fma.rn.ftz.f32 	%f43, %f22, %f22, %f42;
	add.ftz.f32 	%f44, %f41, %f43;
	ld.global.nc.u32 	%r33, [%rd23+4];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	mul.ftz.f32 	%f45, %f25, %f25;
	fma.rn.ftz.f32 	%f46, %f24, %f24, %f45;
	add.ftz.f32 	%f47, %f44, %f46;
	ld.global.nc.u32 	%r35, [%rd23+8];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	mul.ftz.f32 	%f48, %f27, %f27;
	fma.rn.ftz.f32 	%f49, %f26, %f26, %f48;
	add.ftz.f32 	%f50, %f47, %f49;
	ld.global.nc.u32 	%r37, [%rd23+12];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	mul.ftz.f32 	%f51, %f29, %f29;
	fma.rn.ftz.f32 	%f52, %f28, %f28, %f51;
	add.ftz.f32 	%f63, %f50, %f52;
	add.s32 	%r45, %r45, 8;
	add.s64 	%rd23, %rd23, 32;
	add.s32 	%r44, %r44, -8;
	setp.ne.s32 	%p4, %r44, 0;
	@%p4 bra 	$L__BB3_4;

$L__BB3_5:
	setp.eq.s32 	%p5, %r46, 0;
	@%p5 bra 	$L__BB3_8;

	mul.wide.s32 	%rd14, %r45, 2;
	add.s64 	%rd15, %rd14, %rd2;
	shl.b64 	%rd16, %rd15, 1;
	add.s64 	%rd24, %rd1, %rd16;

$L__BB3_7:
	.pragma "nounroll";
	ld.global.nc.u32 	%r39, [%rd24];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	mul.ftz.f32 	%f55, %f54, %f54;
	fma.rn.ftz.f32 	%f56, %f53, %f53, %f55;
	add.ftz.f32 	%f63, %f63, %f56;
	add.s64 	%rd24, %rd24, 4;
	add.s32 	%r46, %r46, -1;
	setp.ne.s32 	%p6, %r46, 0;
	@%p6 bra 	$L__BB3_7;

$L__BB3_8:
	and.b32  	%r41, %r12, 1;
	setp.eq.b32 	%p7, %r41, 1;
	mov.pred 	%p8, 0;
	xor.pred  	%p9, %p7, %p8;
	not.pred 	%p10, %p9;
	shl.b64 	%rd17, %rd2, 1;
	add.s64 	%rd9, %rd1, %rd17;
	@%p10 bra 	$L__BB3_10;

	add.s32 	%r42, %r12, -1;
	mul.wide.s32 	%rd18, %r42, 2;
	add.s64 	%rd19, %rd9, %rd18;
	ld.global.nc.u16 	%rs1, [%rd19];
	// begin inline asm
	{  cvt.f32.f16 %f57, %rs1;}

	// end inline asm
	fma.rn.ftz.f32 	%f63, %f57, %f57, %f63;

$L__BB3_10:
	cvta.to.global.u64 	%rd20, %rd10;
	mul.wide.s32 	%rd21, %r1, 4;
	add.s64 	%rd22, %rd20, %rd21;
	sqrt.approx.ftz.f32 	%f58, %f63;
	st.global.f32 	[%rd22], %f58;

$L__BB3_11:
	ret;

}

